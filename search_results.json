{"requestId":"e536b6868a37ac794067ca5e3b01ebe2","resolvedSearchType":"neural","results":[{"id":"https://www.assemblyai.com/changelog","title":"Changelog - AssemblyAI","url":"https://www.assemblyai.com/changelog","author":null,"text":"Changelog | AssemblyAI\n[![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67b5bd3d9e8ee1a6b2410b9e_AssemblyAI%20Logo.svg)](https://www.assemblyai.com/)\n# Changelog\nFollow along to see weekly accuracy and product improvements.\n[Subscribe to updates](https://www.assemblyai.com/contact/subscribe)[Follow us on Twitter](https://x.com/assemblyai?lang=en)\nDecember 5, 2025\n## Keyterm Prompting Now Available for Universal-Streaming Multilingual\nKeyterm prompting is now in production for multilingual streaming, giving developers the ability to improve accuracy for target words in real-time transcription. This enhancement is live for all users across the Universal-Streaming platform.\nKeyterm prompting enables developers to prioritize specific terminology in transcription results, which is particularly valuable for conversational AI and voice agent use cases where domain-specific accuracy matters. By specifying keywords relevant to your application, you&#x27;ll see improved recognition of critical terms that might otherwise be misheard or misinterpreted.\nTo use Keyterm prompting with Universal-Streaming Multilingual, include a list of keyterms in your connection parameters:\n```\n`CONNECTION\\_PARAMS = {&quot;&quot;sample\\_rate&quot;&quot;:16000,&quot;&quot;speech\\_model&quot;&quot;:&quot;universal-streaming-multilingual&quot;,&quot;&quot;keyterms\\_prompt&quot;&quot;: json.dumps([&quot;Keanu Reeves&quot;,&quot;AssemblyAI&quot;,&quot;Universal-2&quot;])}`\n```\nExpanding Keyterm prompting to Universal-Multilingual Streaming reinforces our commitment to giving developers precise control over recognition results for specialized vocabularies.\n[Learn more in our docs.](https://www.assemblyai.com/docs/universal-streaming/keyterms-prompting#keyterms-prompting-for-universal-streaming)\nDecember 4, 2025\n## Hallucination Rate Reduced for Multilingual Streaming\nWe&#x27;ve improved hallucination detection and reduction across Universal-Multilingual Streaming transcription, resulting in fewer false outputs while maintaining minimal latency impact. This improvement is live for all users.\nLower hallucination rates mean more reliable transcription results out of the box, especially in edge cases where model confidence is uncertain. You&#x27;ll see more accurate, trustworthy outputs without needing to modify existing implementations\nThis improvement is automatic and applies to all new Streaming sessions.\nDecember 3, 2025\n## Transcription Access Now Scoped to Project Level for Uploaded Files\nWe&#x27;ve tightened security controls on pre-recorded file transcription by scoping access to uploaded files within the same project that uploaded them.\nPreviously, API tokens could transcribe files across projects. Now, tokens must belong to the same project that originally uploaded the file to transcribe it. This strengthens your security posture and prevents unintended cross-project access to sensitive audio files.\nThis security enhancement reflects our commitment to protecting your data and giving you granular control over who can access transcriptions within your organization.\nNovember 21, 2025\n## Gemini 3 Pro now available in LLM Gateway\nGoogle&#x27;s latest Gemini 3 Pro model is now available through AssemblyAI&#x27;s LLM Gateway, giving you access to one of the most advanced multimodal models with the same unified API you use for all your other providers.\nWith AssemblyAI&#x27;s LLM Gateway, you can now test Gemini 3 Pro against models from OpenAI, Anthropic, Google, and others without changing your integration—just swap the model parameter and compare responses, latency, and cost across providers in real-time.\n**Available now for all LLM Gateway Users**\nTo get started, simply update the`&quot;model&quot;`parameter in your LLM Gateway request to`&quot;gemini-3-pro-preview&quot;`:\n```\n`importrequestsheaders = {&quot;authorization&quot;:&quot;&quot;&lt;&lt;YOUR\\_API\\_KEY&gt;&gt;&quot;&quot;}response = requests.post(&quot;https://llm-gateway.assemblyai.com/v1/chat/completions&quot;,headers = headers,json = {&quot;model&quot;:&quot;gemini-3-pro-preview&quot;,&quot;messages&quot;: [{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:&quot;What is the capital of France?&quot;}],&quot;&quot;max\\_tokens&quot;&quot;:1000})result = response.json()print(result[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;])`\n```\nAssemblyAI&#x27;s LLM Gateway gives you a single API to access 15+ LLMs from every major provider, with built-in fallbacks, load balancing, and cost tracking. Compare models, optimize for performance or price, and switch providers instantly, all without rewriting code.\n[View our docs and try Gemini 3 Pro in LLM Gateway.](https://www.assemblyai.com/docs/llm-gateway/overview)\n[![Embedded YouTube video](https://img.youtube.com/vi/o6dB7Im-aU0/0.jpg)](https://www.youtube.com/watch?v=o6dB7Im-aU0)\nNovember 21, 2025\n## AssemblyAI Streaming Updates: Multi-Region Infrastructure, Session Controls, and Self-Hosted License Management\n**Self-Hosted Streaming v0.20: License Management Now Available**\nSelf-Hosted Streaming v0.20 now includes built-in license generation and validation, giving enterprises complete control over deployment security and usage tracking. Organizations can manage their speech AI infrastructure with the same compliance controls they expect from enterprise software.\nThe new licensing system enables IT teams to track deployment usage, enforce security policies, and maintain audit trails—critical for regulated industries like healthcare and financial services. License validation happens at startup and can be configured for periodic checks to ensure continuous compliance.\n**Available now for all AssemblyAI Self-Hosted Streaming customers.**\n‍[Contact your account team](https://www.assemblyai.com/contact/sales)to generate licenses for your deployments.\n‍**Multi-Region Streaming: US-East-1 Now Live**\nAssemblyAI&#x27;s Streaming API is now available in`us-east-1`, providing regional redundancy and expanded compute capacity for production workloads. The infrastructure update reduces single-region dependency and prepares the platform for upcoming EU deployment.\nMulti-region availability means contact centers and live captioning applications can maintain service continuity during regional incidents while accessing additional compute capacity for peak usage periods. The architecture changes also enable faster rollout of new regions based on customer demand.\n**Available immediately across all AssemblyAI&#x27;s Streaming API plans.**Traffic is automatically routed to the optimal region based on latency and capacity.\n[Try AssemblyAI’s Streaming API now](https://www.assemblyai.com/dashboard/signup)or[view regional availability](https://www.assemblyai.com/docs/integrations/recall#recall_region-must-be-one-of-us-west-2-us-east-1-eu-central-1-ap-northeast-1).\n‍**Inactivity Timeout Controls for Streaming Sessions**\nAssemlyAI’s Streaming API now supports configurable`inactivity\\_timeout`parameters, giving developers precise control over session duration management. Applications can extend timeout periods for long-running sessions or reduce them to optimize connection costs.\nThe feature enables voice agents and live transcription systems to automatically close idle connections without manual intervention. Contact centers can reduce costs on silent periods while ensuring active calls stay connected. Voice agent developers can keep sessions open longer during natural conversation pauses without manual keep-alive logic.\n**Available now for all AssemblyAI Streaming customers.**Set the`inactivity\\_timeout`parameter (in seconds) when initializing your connection.\nImplementation:\n* Set`inactivity\\_timeout`in your connection parameters\n* Values range from 5 to 3600 seconds\n* Default timeout remains 30 seconds if not specified\n* Available across all pricing tiers\n[View our documentation to learn more.](https://www.assemblyai.com/docs/guides/terminate_realtime_programmatically)\n‍November 21, 2025\n## Streaming Model Update: Enhanced Performance &amp; New Capabilities\nWe&#x27;ve released a new version of our English Streaming model with significant improvements across the board.\n**Performance gains:**\n* 88% better accuracy on short utterances and repeating commands/numbers\n* 12% faster emission latency\n* 7% faster time to complete transcript\n* 4% better accuracy on prompted keyterms\n* 3% better accuracy on accented speech\n**New features:**\n* [Language detection for utterances](https://www.assemblyai.com/docs/universal-streaming/multilingual-transcription#language-detection)(Multilingual model only) –Get language output for each utterance to feed downstream processes like LLMs\n* [Dynamic keyterms prompting](https://www.assemblyai.com/docs/universal-streaming/keyterms-prompting#dynamic-keyterms-prompting)– Update your keyterms list mid-stream to improve accuracy on context you discover during the conversation\nNovember 18, 2025\n## LeMUR Deprecation\nLeMUR will be**deprecated on March 31, 2026**and will no longer work after this date.\nUsers will need to migrate to LLM Gateway by that date for continued access to language model capabilities and benefit from an expanded model selection as well as better performance.\nFor more information, check out our[documentation on LLM Gateway](https://www.assemblyai.com/docs/llm-gateway/overview)and our[LeMUR to LLM Gateway migration guide](https://www.assemblyai.com/docs/llm-gateway/migration-from-lemur).\n‍November 12, 2025\n## Universal Multilingual Streaming\nWe&#x27;ve launched Universal Multilingual Streaming, enabling low-latency transcription across multiple languages without compromising accuracy.\n**Key Features**\n* **6 languages:**English, Spanish, French, German, Italian, Portuguese\n* **Industry-leading accuracy****:**11.77% average WER across real-world audio\n* **$0.15/hour flat:**No language surcharges\n* **Production-ready****:**Punctuation, capitalization, and intelligent endpointing included\n**Use Cases**\n* Voice agents serving international customers\n* Multi-language contact centers\n* Cross-border meeting transcription\n* Healthcare systems serving diverse communities\nNo complex language detection pipelines or multiple vendor integrations required. See our[documentation](https://www.assemblyai.com/docs/speech-to-text/universal-streaming/multilingual-transcription)for implementation details.\n‍November 5, 2025\n## Deprecation of V2 Streaming\nOur legacy streaming endpoint (`/v2/realtime/ws`) will be**deprecated on January 31, 2026**, and will no longer work after this date.\nUsers will need to migrate to Universal-Streaming before the deadline to avoid service interruption.\n**Why upgrade?**\n- Higher accuracy\n- Lower latency\n- Intelligent endpointing\n-[Multilingual support](https://www.assemblyai.com/docs/universal-streaming/multilingual-transcription)\n- Lower pricing ($0.15/hour)\nFor more information, check out our[documentation on Universal-Streaming](https://www.assemblyai.com/docs/universal-streaming)and our[V2 to V3 migration guide](https://www.assemblyai.com/docs/guides/v2_to_v3_migration).\n‍October 23, 2025\n## Claude 3.5 &amp; 3.7 Sonnet Sunset\nAs previously announced, we will be sunsetting Claude 3.5 Sonnet and 3.7 Sonnet for LeMUR on October 29th. After this date, requests made using Claude 3.5 and 3.7 Sonnet will return errors.\nIf you are using this model, we recommend switching to**Claude 4 Sonnet**, which is more performant than Claude 3.5 and 3.7 Sonnet. You can switch models by setting the`final\\_model`[](https://www.assemblyai.com/docs/lemur/customize-parameters)parameter to`anthropic/claude-sonnet-4-20250514`\n‍October 22, 2025\n## New Voice AI Tools and Model Updates\nIntroducing new tools and model updates to help you build, deploy, and scale Voice AI applications:\n**Speech Understanding**: Advanced speaker identification, custom formatting rules, and translation let you transform raw transcripts into structured data instantly\n**LLM Gateway**: One API for your entire voice-to-intelligence pipeline with integrated access to GPT, Claude, Gemini, and others.\n**Voice AI Guardrails:**PII redaction in 50+ languages, profanity filtering, and content moderation.\n**Model Enhancements:**\n* Automatically code-switch between 99 languages, with 64% fewer speaker counting errors\n* Up to 57% accuracy improvements on critical terms with 1,000-word context-aware prompting\nRead more about these tools in our[blog](https://www.assemblyai.com/blog/introducing-new-products-and-model-updates)and check out our[documentation](https://www.assemblyai.com/docs)for more information.\n‍October 15, 2025\n## Speaker Diarization Update\nWe&#x27;ve shipped significant improvements to speaker count accuracy on Universal and Slam-1:\n* 36% more accurate speaker detection for pre-recorded files\n* Fewer false positives and missed speakers - the model now consistently identifies the correct number of participants\nOctober 1, 2025\n## Slam-1 bugfixes\nFix released to address hallucinations occasionally produced in Slam-1 transcriptions.\n**Slam-1 Timestamps:**\n* Fixed issue where sentences separated by silence were sometimes incorrectly combined due to inaccurate timestamps.\n* Fix released to reduce occasional timestamp inconsistencies.\n‍September 19, 2025\n## Universal-Streaming Improvements\nWe&#x27;ve released updates to our Universal-Streaming model, bringing significant performance improvements across the board.\n‍**What&#x27;s better:**\n* **Overall accuracy**: 3% improvement in general transcription accuracy\n* **Accented speech**: 4% better recognition for speakers with accents\n* **Conversation Intelligence segments**: 4% improvement in conversation intelligence use cases\n* **Proper nouns**: 7% better at recognizing names, brands, and places\n* **Repeated words**: 21% improvement when speakers repeat themselves\n* **Speed**: 20ms faster response time for even lower latency\n* **Keyterms Prompting**: Up to 66% better recognition of your custom terms\n‍September 18, 2025\n## Keyterms Prompt for Universal (Beta) and PII Redaction Updates; bugfix\nThe`keyterms\\_prompt`parameter can now be used with Universal for pre-recorded audio transcription, ensuring accurate recognition of product names, people, and industry terms. This feature is in Beta and only available for English files. For more information, please refer to our[documentation](https://www.assemblyai.com/docs/speech-to-text/pre-recorded-audio/improving-transcript-accuracy#using-universal-beta).\n[PII Audio Redaction](https://www.assemblyai.com/docs/audio-intelligence/pii-redaction#create-redacted-audio-files)is now available for files processed via the EU endpoint.\nPII Redaction now[supports](https://www.assemblyai.com/docs/speech-to-text/pre-recorded-audio/supported-languages)additional languages: Afrikaans, Bengali, and Thai.\nFixed issue where occasionally Slam-1 incorrectly inserted new lines in transcripts.\nSeptember 12, 2025\n## Playground Updates; bugfixes\n**LeMUR Integration**: LeMUR is now available via the Playground, enabling enhanced language understanding and processing capabilities\n**Account-Based Playground**: The Playground is now attached to individual accounts, allowing users to track their transcription history\n![](https://cdn.prod.website-files.com/67a1e6de2f2eab2e125f8b9a/68c400ec35883ee280a6981f_in_app_playground.png)\n**Speaker Diarization**: Fixed occasional errors when using speaker diarization with non-English languages\n**Slam-1**:\n* Resolved errors caused by multiple consecutive punctuation symbols (e.g., &#x27;??&#x27; or &#x27;!!&#x27;)\n* Fixed timestamp adjustments that were causing shifts in word ordering\n* Reduced hallucinations in transcript text output\n**Text Formatting**: Released a fix that mitigates occasional punctuation and casing inconsistencies in transcriptions\nSeptember 11, 2025\n## Keyterms Prompting for Universal-Streaming\nVoice AI finally understands the words that matter most to your business - product names, people, industry terms - with perfect accuracy in real-time.\n**The impact:**\n* 21% better accuracy than leading alternatives\n* 67% lower cost ($0.04/hour)\n* No impact on streaming latency\n**Who wins:**Restaurant ordering bots that never mishear menu items. Medical schedulers that get doctor names right. Meeting tools with searchable, accurate transcripts.\nInclude a maximum of 100 keyterms per session. For more information about this new feature and implementation, please refer to our[blog](https://www.assemblyai.com/blog/streaming-keyterms-prompting)and[documentation](https://www.assemblyai.com/docs/speech-to-text/universal-streaming/keyterms-prompting).\nAugust 26, 2025\n## Universal Language Expansion\nUniversal now delivers production-ready accuracy and features across 99 languages through a single, unified endpoint.\nWhat&#x27;s new:\n* **Expanded language detection**– Automatically detects all 99 languages (up from 17)\n* **Global speaker diarization**– Identify speakers in 95 languages with precision\n* **Superior performance**– Experience 2-3x faster processing for languages like Spanish, French, and German\n* **Customizable language detection**– Set expected languages and fallback options tailored to your specific use case\nEnable comprehensive language detection with just one parameter and no complex integration required. Check out our[blog](https://www.assemblyai.com/blog/99-languages)and[documentation](https://www.assemblyai.com/docs/speech-to-text/pre-recorded-audio/automatic-language-detection)to explore Universal&#x27;s capabilities.\nAugust 21, 2025\n## Streaming Update; bugfix\nAdded Voice Activity Detection (VAD) to our endpointing model for more accurate detection of ongoing speech. Interruptions are reduced by nearly 100%, while still accurately predicting user end of turns. This feature is now natively integrated into the model and works automatically so no setup is required.\nFixed a bug where using Slam-1 with speaker diarization occasionally resulted in a server error.\nAugust 7, 2025\n## Dashboard Region Rates Toggle\nAdded a toggle on the dashboard under the[Billing tab in the Account section](https://www.assemblyai.com/dashboard/account/billing)to switch the view between US and EU rates.\n![](https://cdn.prod.website-files.com/67a1e6de2f2eab2e125f8b9a/689470d100407f5ca1d0d9cd_dashboard_region_rates_toggle.png)\n[\nLoad more\n](?3b7e7275_page=2)\n1 / 10","image":"https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67e5a4788594c44228a2ed1a_AAI.com%20-%20Meta%20image.avif","favicon":"https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67e119e1e653ef1ce8e17a50_aai-favicon-32x32.png"},{"id":"https://assemblyai.com/docs/llm-gateway/overview","title":"LLM Gateway Overview | AssemblyAI | Documentation","url":"https://assemblyai.com/docs/llm-gateway/overview","author":null,"text":"LLM Gateway Overview | AssemblyAI | Documentation\nGemini 3 Pro now available in LLM Gateway![Learn more](https://www.assemblyai.com/docs/llm-gateway/overview)\nSearch\n/\nAsk AI\n[Documentation](https://www.assemblyai.com/docs)[API Reference](https://www.assemblyai.com/docs/api-reference/overview)[Cookbooks](https://www.assemblyai.com/docs/guides)[FAQ](https://www.assemblyai.com/docs/faq)[Playground](https://www.assemblyai.com/playground)[Changelog](https://www.assemblyai.com/changelog)[Roadmap](https://www.assemblyai.com/roadmap)\n[Documentation](https://www.assemblyai.com/docs)[API Reference](https://www.assemblyai.com/docs/api-reference/overview)[Cookbooks](https://www.assemblyai.com/docs/guides)[FAQ](https://www.assemblyai.com/docs/faq)[Playground](https://www.assemblyai.com/playground)[Changelog](https://www.assemblyai.com/changelog)[Roadmap](https://www.assemblyai.com/roadmap)\n* Getting started\n* [Overview](https://www.assemblyai.com/docs)\n* [Transcribe a pre-recorded audio file](https://www.assemblyai.com/docs/getting-started/transcribe-an-audio-file)\n* [Transcribe streaming audio](https://www.assemblyai.com/docs/getting-started/transcribe-streaming-audio)\n* Models\n* [Models](https://www.assemblyai.com/docs/getting-started/models)\n* [Pre-recorded Speech-to-text](https://www.assemblyai.com/docs/pre-recorded-audio)\n* [Streaming Speech-to-text](https://www.assemblyai.com/docs/universal-streaming)\n* [Evaluations](https://www.assemblyai.com/docs/evaluations)\n* Voice AI Platform\n* Speech Understanding\n* Guardrails\n* LLM Gateway\n* [Deployment](https://www.assemblyai.com/docs/deployment)\n* Security\n* [Integrations](https://www.assemblyai.com/docs/integrations)\n* Build with AssemblyAI\n* Build a Voice Agent\n* [Build a Meeting Notetaker](https://www.assemblyai.com/docs/meeting-notetaker-best-practices)\n* [Build a Medical Scribe](https://www.assemblyai.com/docs/medical-scribe-best-practices)\n* Migration guides\n[Sign In](https://www.assemblyai.com/dashboard/login)\nLight\nOn this page\n* [Overview](#overview)\n* [Available models](#available-models)\n* [Anthropic Claude](#anthropic-claude)\n* [OpenAI GPT](#openai-gpt)\n* [Google Gemini](#google-gemini)\n* [Select a model](#select-a-model)\n* [Next steps](#next-steps)\n[Voice AI Platform](https://www.assemblyai.com/docs/speech-understanding/speaker-identification)[LLM Gateway](https://www.assemblyai.com/docs/llm-gateway/overview)\n# LLM Gateway Overview\nCopy page\n## Overview\n**AssemblyAI’s LLM Gateway**is a unified interface that allows you to connect with multiple LLM providers including Claude, GPT, and Gemini. You can use the LLM Gateway to build sophisticated AI applications through a single API.\nThe LLM Gateway provides access to 15+ models across major AI providers with support for:\n* **Basic Chat Completions**- Simple request/response interactions\n* **Multi-turn Conversations**- Maintain context across multiple exchanges\n* **Tool/Function Calling**- Enable models to execute custom functions\n* **Agentic Workflows**- Multi-step reasoning with automatic tool chaining\n* **Unified Interface**- One API for Claude, GPT, Gemini, and more## Available models\n### Anthropic Claude\n|Model|Parameter|Latency per 10,000 tokens|[LMArena Score](https://lmarena.ai/leaderboard)|Description|Retention Policy|Anthropic Model Training|\n**Claude 4.5 Sonnet**|`claude-sonnet-4-5-20250929`|10.1s|1444|Claude’s best model for complex agents and coding|We use this model through Amazon Bedrock. Amazon Bedrock doesn’t store or log your prompts and completions. Amazon Bedrock doesn’t use your prompts and completions to train any AWS models and doesn’t distribute them to third parties. See[here](https://docs.aws.amazon.com/bedrock/latest/userguide/data-protection.html)for more information on Amazon Bedrock data protection policies.\\*|AssemblyAI has opted out of model training with all LLM Gateway providers.|\n**Claude 4 Sonnet**|`claude-sonnet-4-20250514`|7.1s|1389|High-performance model|We use this model through Amazon Bedrock. Amazon Bedrock doesn’t store or log your prompts and completions. Amazon Bedrock doesn’t use your prompts and completions to train any AWS models and doesn’t distribute them to third parties. See[here](https://docs.aws.amazon.com/bedrock/latest/userguide/data-protection.html)for more information on Amazon Bedrock data protection policies.\\*|AssemblyAI has opted out of model training with all LLM Gateway providers.|\n**Claude 4 Opus**|`claude-opus-4-20250514`|15.4s|1412|Claude’s previous flagship model|We use this model through Amazon Bedrock. Amazon Bedrock doesn’t store or log your prompts and completions. Amazon Bedrock doesn’t use your prompts and completions to train any AWS models and doesn’t distribute them to third parties. See[here](https://docs.aws.amazon.com/bedrock/latest/userguide/data-protection.html)for more information on Amazon Bedrock data protection policies.\\*|AssemblyAI has opted out of model training with all LLM Gateway providers.|\n**Claude 4.5 Haiku**|`claude-haiku-4-5-20251001`|4.6s|1402|Claude’s fastest and most intelligent Haiku model|We use this model through Amazon Bedrock. Amazon Bedrock doesn’t store or log your prompts and completions. Amazon Bedrock doesn’t use your prompts and completions to train any AWS models and doesn’t distribute them to third parties. See[here](https://docs.aws.amazon.com/bedrock/latest/userguide/data-protection.html)for more information on Amazon Bedrock data protection policies.\\*|AssemblyAI has opted out of model training with all LLM Gateway providers.|\n**Claude 3.5 Haiku**|`claude-3-5-haiku-20241022`|5.4s|1322|Fast and efficient model with strong performance|We use this model through Amazon Bedrock. Amazon Bedrock doesn’t store or log your prompts and completions. Amazon Bedrock doesn’t use your prompts and completions to train any AWS models and doesn’t distribute them to third parties. See[here](https://docs.aws.amazon.com/bedrock/latest/userguide/data-protection.html)for more information on Amazon Bedrock data protection policies.\\*|AssemblyAI has opted out of model training with all LLM Gateway providers.|\n**Claude 3.0 Haiku**|`claude-3-haiku-20240307`|4.8s|1262|Fast and compact model for near-instant responsiveness|We use this model through Amazon Bedrock. Amazon Bedrock doesn’t store or log your prompts and completions. Amazon Bedrock doesn’t use your prompts and completions to train any AWS models and doesn’t distribute them to third parties. See[here](https://docs.aws.amazon.com/bedrock/latest/userguide/data-protection.html)for more information on Amazon Bedrock data protection policies.\\*|AssemblyAI has opted out of model training with all LLM Gateway providers.|\n##### \\*If Amazon Bedrock fails, for non-EU customers we may send your request to the Anthropic API, where we have 0-day retention configured. Please see Anthropic’s commercial terms[here](https://www.anthropic.com/legal/commercial-terms).\n### OpenAI GPT\n|Model|Parameter|Latency per 10,000 tokens|[LMArena Score](https://lmarena.ai/leaderboard)|Description|Retention Policy|OpenAI Model Training|\n**GPT-5**|`gpt-5`|18.9s|1425|OpenAI’s best model for coding and agentic tasks across domains|Abuse monitoring retains logs for 30 days. If you require ZDR, please use Anthropic or Google models.|AssemblyAI has opted out of model training with all LLM Gateway providers.|\n**GPT-5 nano**|`gpt-5-nano`|11.2s|1338|OpenAI’s fastest, most cost-efficient version of GPT-5|Abuse monitoring retains logs for 30 days. If you require ZDR, please use Anthropic or Google models.|AssemblyAI has opted out of model training with all LLM Gateway providers.|\n**GPT-5 mini**|`gpt-5-mini`|21.9s|1393|A faster, cost-efficient version of GPT-5 for well-defined tasks|Abuse monitoring retains logs for 30 days. If you require ZDR, please use Anthropic or Google models.|AssemblyAI has opted out of model training with all LLM Gateway providers.|\n**GPT-4.1**|`gpt-4.1`|12.6s|1412|OpenAI’s smartest non-reasoning model|Abuse monitoring retains logs for 30 days. If you require ZDR, please use Anthropic or Google models.|AssemblyAI has opted out of model training with all LLM Gateway providers.|\n**gpt-oss-120b**|`gpt-oss-120b`|10.5s|1352|OpenAI’s most powerful open-weight model|We use this model through Amazon Bedrock. Amazon Bedrock doesn’t store or log your prompts and completions. Amazon Bedrock doesn’t use your prompts and completions to train any AWS models and doesn’t distribute them to third parties. See[here](https://docs.aws.amazon.com/bedrock/latest/userguide/data-protection.html)for more information on Amazon Bedrock data protection policies.|AssemblyAI has opted out of model training with all LLM Gateway providers.|\n**gpt-oss-20b**|`gpt-oss-20b`|4.2s|1318|Medium-sized open-weight model for low latency|We use this model through Amazon Bedrock. Amazon Bedrock doesn’t store or log your prompts and completions. Amazon Bedrock doesn’t use your prompts and completions to train any AWS models and doesn’t distribute them to third parties. See[here](https://docs.aws.amazon.com/bedrock/latest/userguide/data-protection.html)for more information on Amazon Bedrock data protection policies.|AssemblyAI has opted out of model training with all LLM Gateway providers.|\n### Google Gemini\n|Model|Parameter|Latency per 10,000 tokens|[LMArena Score](https://lmarena.ai/leaderboard)|Description|Retention Policy|Google Model Training|\n**Gemini 3 Pro Preview**|`gemini-3-pro-preview`|10.0s|1495|Gemini’s most powerful agentic and vibe-coding model, delivering richer visuals and deeper interactivity|ZDR (see Google’s policy[here](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/vertex-ai-zero-data-retention)for more information on how Google defines ZDR)|AssemblyAI has opted out of model training with all LLM Gateway providers.|\n**Gemini 2.5 Pro**|`gemini-2.5-pro`|13.9s|1451|Gemini’s state-of-the-art thinking model, capable of reasoning over complex problems|ZDR (see Google’s policy[here](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/vertex-ai-zero-data-retention)for more information on how Google defines ZDR)|AssemblyAI has opted out of model training with all LLM Gateway providers.|\n**Gemini 2.5 Flash**|`gemini-2.5-flash`|8.3s|1407|Gemini’s best model in terms of price-performance, offering well-rounded capabilities|ZDR (see Google’s policy[here](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/vertex-ai-zero-data-retention)for more information on how Google defines ZDR)|AssemblyAI has opted out of model training with all LLM Gateway providers.|\n**Gemini 2.5 Flash-Lite**|`gemini-2.5-flash-lite`|1.6s|1375|Gemini’s fastest flash model optimized for cost-efficiency and high throughput|ZDR (see Google’s policy[here](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/vertex-ai-zero-data-retention)for more information on how Google defines ZDR)|AssemblyAI has opted out of model training with all LLM Gateway providers.|\nUnsure which model to choose?\n* Consider Claude models for nuanced reasoning and complex instructions\n* Consider GPT models for code generation and structured outputs\n* Consider Gemini models for cost-effective high-volume applications\n##### Head to[our Playground](https://www.assemblyai.com/dashboard/playground)to\ntest out LLM Gateway without having to write any code!\n## Select a model\nYou can specify which model to use in your request by setting the`model`parameter. Here are examples showing how to use Claude 4.5 Sonnet:\n###### Python\n###### JavaScript\n```\n`\n1|import requests|\n2||\n3|headers = {|\n4|&quot;&quot;authorization&quot;&quot;: &quot;&quot;&lt;&lt;YOUR\\_API\\_KEY&gt;&gt;&quot;&quot;|\n5|}|\n6||\n7|response = requests.post(|\n8|&quot;https://llm-gateway.assemblyai.com/v1/chat/completions&quot;,|\n9|headers = headers,|\n10|json = {|\n11|&quot;model&quot;: &quot;claude-sonnet-4-5-20250929&quot;,|\n12|&quot;messages&quot;: [|\n13|{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the capital of France?&quot;}|\n14|],|\n15|&quot;&quot;max\\_tokens&quot;&quot;: 1000|\n16|}|\n17|)|\n18||\n19|result = response.json()|\n20|print(result[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;])|\n`\n```\nSimply change the`model`parameter to use any of the available models listed in the[Available models](https://www.assemblyai.com/docs/llm-gateway/overview#available-models)section above.\n## Next steps\n* [Basic Chat Completions](https://www.assemblyai.com/docs/llm-gateway/chat-completions)- Learn how to send simple messages and receive responses\n* [Multi-turn Conversations](https://www.assemblyai.com/docs/llm-gateway/conversations)- Maintain context across multiple exchanges\n* [Tool Calling](https://www.assemblyai.com/docs/llm-gateway/tool-calling)- Enable models to execute custom functions\n* [Agentic Workflows](https://www.assemblyai.com/docs/llm-gateway/agentic-workflows)- Build multi-step reasoning applications\n##### The LLM Gateway API is separate from the Speech-to-Text and Audio Intelligence\nAPIs. It provides a unified interface to work with large language models\nacross multiple providers.\n[![Logo](https://files.buildwithfern.com/assemblyai.docs.buildwithfern.com/docs/2025-12-11T06:33:48.914Z/assets/logo-light.svg)![Logo](https://files.buildwithfern.com/assemblyai.docs.buildwithfern.com/docs/2025-12-11T06:33:48.914Z/assets/AssemblyAI_White.svg)](https://www.assemblyai.com/)","image":"https://files.buildwithfern.com/assemblyai.docs.buildwithfern.com/docs/2025-12-11T06:33:48.914Z/assets/logo-light.svg","favicon":"https://files.buildwithfern.com/assemblyai.docs.buildwithfern.com/docs/2025-12-11T06:33:48.914Z/assets/favicon.png"},{"id":"https://ai.google.dev/gemini-api/docs/models","title":"Gemini models | Gemini API | Google AI for Developers","url":"https://ai.google.dev/gemini-api/docs/models","author":null,"text":"Gemini models | Gemini API | Google AI for Developers[Skip to main content](#main-content)\n[![Gemini API](https://ai.google.dev/_static/googledevai/images/gemini-api-logo.svg)](https://ai.google.dev/)\n* /\n* English\n* Deutsch\n* Español –América Latina\n* Français\n* Indonesia\n* Italiano\n* Polski\n* Português –Brasil\n* Shqip\n* Tiếng Việt\n* Türkçe\n* Русский* עברית* العربيّة* فارسی* हिंदी* বাংলা* ภาษาไทย* 中文–简体* 中文–繁體* 日本語* 한국어[Get API key](https://aistudio.google.com/apikey)[Cookbook](https://github.com/google-gemini/cookbook)[Community](https://discuss.ai.google.dev/c/gemini-api/)Sign in\nGemini 3 Flash is here.[Try it for free in Google AI Studio](https://aistudio.google.com?model=gemini-3-flash-preview).\n* [Home](https://ai.google.dev/)\n* [Gemini API](https://ai.google.dev/gemini-api)\n* [Docs](https://ai.google.dev/gemini-api/docs)\nSend feedback# Gemini models\nWe have updated our[Terms of Service](https://ai.google.dev/gemini-api/terms).\nOUR MOST INTELLIGENT MODEL\n## Gemini 3 Pro\nThe best model in the world for multimodal understanding, and our most powerful agentic and vibe-coding model yet, delivering richer visuals and deeper interactivity, all built on a foundation of state-of-the-art reasoning.\n### Expand to learn more\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-3-pro-preview)\n#### Model details\n### Gemini 3 Pro Preview\n|Property|Description|\nid\\_cardModel code|`gemini-3-pro-preview`|\nsaveSupported data types|\n**Inputs**\nText, Image, Video, Audio, and PDF\n**Output**\nText\n|\ntoken\\_autoToken limits[[\\*]](https://ai.google.dev/gemini-api/docs/tokens)|\n**Input token limit**\n1,048,576\n**Output token limit**\n65,536\n|\nhandymanCapabilities|\n**Audio generation**\nNot supported\n**Batch API**\nSupported\n**Caching**\nSupported\n**Code execution**\nSupported\n**File search**\nSupported\n**Function calling**\nSupported\n**Grounding with Google Maps**\nNot supported\n**Image generation**\nNot supported\n**Live API**\nNot supported\n**Search grounding**\nSupported\n**Structured outputs**\nSupported\n**Thinking**\nSupported\n**URL context**\nSupported\n|\n123Versions|\nRead the[model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions)for more details.\n* `Preview: gemini-3-pro-preview`\n|\ncalendar\\_monthLatest update|November 2025|\ncognition\\_2Knowledge cutoff|January 2025|\n### Gemini 3 Pro Image Preview\n|Property|Description|\nid\\_cardModel code|`gemini-3-pro-image-preview`|\nsaveSupported data types|\n**Inputs**\nImage and Text\n**Output**\nImage and Text\n|\ntoken\\_autoToken limits[[\\*]](https://ai.google.dev/gemini-api/docs/tokens)|\n**Input token limit**\n65,536\n**Output token limit**\n32,768\n|\nhandymanCapabilities|\n**Audio generation**\nNot supported\n**Batch API**\nSupported\n**Caching**\nNot supported\n**Code execution**\nNot supported\n**File search**\nNot supported\n**Function calling**\nNot supported\n**Grounding with Google Maps**\nNot supported\n**Image generation**\nSupported\n**Live API**\nNot supported\n**Search grounding**\nSupported\n**Structured outputs**\nSupported\n**Thinking**\nSupported\n**URL context**\nNot supported\n|\n123Versions|\nRead the[model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions)for more details.\n* `Preview: gemini-3-pro-image-preview`\n|\ncalendar\\_monthLatest update|November 2025|\ncognition\\_2Knowledge cutoff|January 2025|\nOUR MOST INTELLIGENT MODEL\n## Gemini 3 Flash\nOur most intelligent model built for speed, combining frontier intelligence with superior search and grounding.\n### Expand to learn more\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-3-flash-preview)\n#### Model details\n### Gemini 3 Flash Preview\n|Property|Description|\nid\\_cardModel code|`gemini-3-flash-preview`|\nsaveSupported data types|\n**Inputs**\nText, Image, Video, Audio, and PDF\n**Output**\nText\n|\ntoken\\_autoToken limits[[\\*]](https://ai.google.dev/gemini-api/docs/tokens)|\n**Input token limit**\n1,048,576\n**Output token limit**\n65,536\n|\nhandymanCapabilities|\n**Audio generation**\nNot supported\n**Batch API**\nSupported\n**Caching**\nSupported\n**Code execution**\nSupported\n**File search**\nSupported\n**Function calling**\nSupported\n**Grounding with Google Maps**\nNot supported\n**Image generation**\nNot supported\n**Live API**\nNot supported\n**Search grounding**\nSupported\n**Structured outputs**\nSupported\n**Thinking**\nSupported\n**URL context**\nSupported\n|\n123Versions|\nRead the[model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions)for more details.\n* `Preview: gemini-3-flash-preview`\n|\ncalendar\\_monthLatest update|December 2025|\ncognition\\_2Knowledge cutoff|January 2025|\nFAST AND INTELLIGENT\n## Gemini 2.5 Flash\nOur best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases.\n### Expand to learn more\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-flash)\n#### Model details\n### Gemini 2.5 Flash\n|Property|Description|\nid\\_cardModel code|`gemini-2.5-flash`|\nsaveSupported data types|\n**Inputs**\nText, images, video, audio\n**Output**\nText\n|\ntoken\\_autoToken limits[[\\*]](https://ai.google.dev/gemini-api/docs/tokens)|\n**Input token limit**\n1,048,576\n**Output token limit**\n65,536\n|\nhandymanCapabilities|\n**Audio generation**\nNot supported\n**Batch API**\nSupported\n**Caching**\nSupported\n**Code execution**\nSupported\n**File search**\nSupported\n**Function calling**\nSupported\n**Grounding with Google Maps**\nSupported\n**Image generation**\nNot supported\n**Live API**\nNot supported\n**Search grounding**\nSupported\n**Structured outputs**\nSupported\n**Thinking**\nSupported\n**URL context**\nSupported\n|\n123Versions|\nRead the[model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions)for more details.\n* Stable:`gemini-2.5-flash`\n|\ncalendar\\_monthLatest update|June 2025|\ncognition\\_2Knowledge cutoff|January 2025|\n### Gemini 2.5 Flash Preview\n|Property|Description|\nid\\_cardModel code|`gemini-2.5-flash-preview-09-2025`|\nsaveSupported data types|\n**Inputs**\nText, images, video, audio\n**Output**\nText\n|\ntoken\\_autoToken limits[[\\*]](https://ai.google.dev/gemini-api/docs/tokens)|\n**Input token limit**\n1,048,576\n**Output token limit**\n65,536\n|\nhandymanCapabilities|\n**Audio generation**\nNot supported\n**Batch API**\nSupported\n**Caching**\nSupported\n**Code execution**\nSupported\n**File search**\nSupported\n**Function calling**\nSupported\n**Grounding with Google Maps**\nNot supported\n**Image generation**\nNot supported\n**Live API**\nNot supported\n**Search grounding**\nSupported\n**Structured outputs**\nSupported\n**Thinking**\nSupported\n**URL Context**\nSupported\n|\n123Versions|\nRead the[model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions)for more details.\n* Preview:`gemini-2.5-flash-preview-09-2025`\n|\ncalendar\\_monthLatest update|September 2025|\ncognition\\_2Knowledge cutoff|January 2025|\n### Gemini 2.5 Flash Image\n|Property|Description|\nid\\_cardModel code|`gemini-2.5-flash-image`|\nsaveSupported data types|\n**Inputs**\nImages and text\n**Output**\nImages and text\n|\ntoken\\_autoToken limits[[\\*]](https://ai.google.dev/gemini-api/docs/tokens)|\n**Input token limit**\n65,536\n**Output token limit**\n32,768\n|\nhandymanCapabilities|\n**Audio generation**\nNot supported\n**Batch API**\nSupported\n**Caching**\nSupported\n**Code execution**\nNot Supported\n**File search**\nNot Supported\n**Function calling**\nNot supported\n**Grounding with Google Maps**\nNot supported\n**Image generation**\nSupported\n**Live API**\nNot Supported\n**Search grounding**\nNot Supported\n**Structured outputs**\nSupported\n**Thinking**\nNot Supported\n**URL context**\nNot supported\n|\n123Versions|\nRead the[model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions)for more details.\n* Stable:`gemini-2.5-flash-image`\n* Deprecated:`gemini-2.5-flash-image-preview`\n|\ncalendar\\_monthLatest update|October 2025|\ncognition\\_2Knowledge cutoff|June 2025|\n### Gemini 2.5 Flash Live\n|Property|Description|\nid\\_cardModel code|`gemini-2.5-flash-native-audio-preview-12-2025`|\nsaveSupported data types|\n**Inputs**\nAudio, video, text\n**Output**\nAudio and text\n|\ntoken\\_autoToken limits[[\\*]](https://ai.google.dev/gemini-api/docs/tokens)|\n**Input token limit**\n131,072\n**Output token limit**\n8,192\n|\nhandymanCapabilities|\n**Audio generation**\nSupported\n**Batch API**\nNot supported\n**Caching**\nNot supported\n**Code execution**\nNot supported\n**File search**\nNot Supported\n**Function calling**\nSupported\n**Grounding with Google Maps**\nNot supported\n**Image generation**\nNot supported\n**Live API**\nSupported\n**Search grounding**\nSupported\n**Structured outputs**\nNot supported\n**Thinking**\nSupported\n**URL context**\nNot supported\n|\n123Versions|\nRead the[model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions)for more details.\n* Preview:`gemini-2.5-flash-native-audio-preview-12-2025`\n* Preview:`gemini-2.5-flash-native-audio-preview-09-2025`\n|\ncalendar\\_monthLatest update|September 2025|\ncognition\\_2Knowledge cutoff|January 2025|\n### Gemini 2.5 Flash TTS\n|Property|Description|\nid\\_cardModel code|`gemini-2.5-flash-preview-tts`|\nsaveSupported data types|\n**Inputs**\nText\n**Output**\nAudio\n|\ntoken\\_autoToken limits[[\\*]](https://ai.google.dev/gemini-api/docs/tokens)|\n**Input token limit**\n8,192\n**Output token limit**\n16,384\n|\nhandymanCapabilities|\n**Audio generation**\nSupported\n**Batch API**\nSupported\n**Caching**\nNot supported\n**Code execution**\nNot supported\n**File search**\nNot Supported\n**Function calling**\nNot supported\n**Grounding with Google Maps**\nNot supported\n**Image generation**\nNot supported\n**Live API**\nNot supported\n**Search grounding**\nNot supported\n**Structured outputs**\nNot supported\n**Thinking**\nNot supported\n**URL context**\nNot supported\n|\n123Versions|\nRead the[model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions)for more details.\n* `gemini-2.5-flash-preview-tts`\n|\ncalendar\\_monthLatest update|December 2025|\nULTRA FAST\n## Gemini 2.5 Flash-Lite\nOur fastest flash model optimized for cost-efficiency and high throughput.\n### Expand to learn more\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-flash-lite)\n#### Model details\n### Gemini 2.5 Flash-Lite\n|Property|Description|\nid\\_cardModel code|`gemini-2.5-flash-lite`|\nsaveSupported data types|\n**Inputs**\nText, image, video, audio, PDF\n**Output**\nText\n|\ntoken\\_autoToken limits[[\\*]](https://ai.google.dev/gemini-api/docs/tokens)|\n**Input token limit**\n1,048,576\n**Output token limit**\n65,536\n|\nhandymanCapabilities|\n**Audio generation**\nNot supported\n**Batch API**\nSupported\n**Caching**\nSupported\n**Code execution**\nSupported\n**File search**\nSupported\n**Function calling**\nSupported\n**Grounding with Google Maps**\nSupported\n**Image generation**\nNot supported\n**Live API**\nNot supported\n**Search grounding**\nSupported\n**Structured outputs**\nSupported\n**Thinking**\nSupported\n**URL context**\nSupported\n|\n123Versions|\nRead the[model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions)for more details.\n* Stable:`gemini-2.5-flash-lite`\n|\ncalendar\\_monthLatest update|July 2025|\ncognition\\_2Knowledge cutoff|January 2025|\n### Gemini 2.5 Flash-Lite Preview\n|Property|Description|\nid\\_cardModel code|`gemini-2.5-flash-lite-preview-09-2025`|\nsaveSupported data types|\n**Inputs**\nText, image, video, audio, PDF\n**Output**\nText\n|\ntoken\\_autoToken limits[[\\*]](https://ai.google.dev/gemini-api/docs/tokens)|\n**Input token limit**\n1,048,576\n**Output token limit**\n65,536\n|\nhandymanCapabilities|\n**Audio generation**\nNot supported\n**Batch API**\nSupported\n**Caching**\nSupported\n**Code execution**\nSupported\n**File search**\nSupported\n**Function calling**\nSupported\n**Grounding with Google Maps**\nNot supported\n**Image generation**\nNot supported\n**Live API**\nNot supported\n**Search grounding**\nSupported\n**Structured outputs**\nSupported\n**Thinking**\nSupported\n**URL context**\nSupported\n|\n123Versions|\nRead the[model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions)for more details.\n* Preview:`gemini-2.5-flash-lite-preview-09-2025`\n|\ncalendar\\_monthLatest update|September 2025|\ncognition\\_2Knowledge cutoff|January 2025|\nOUR ADVANCED THINKING MODEL\n## Gemini 2.5 Pro\nOur state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.\n### Expand to learn more\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro)\n#### Model details\n### Gemini 2.5 Pro\n|Property|Description|\nid\\_cardModel code|`gemini-2.5-pro`|\nsaveSupported data types|\n**Inputs**\nAudio, images, video, text, and PDF\n**Output**\nText\n|\ntoken\\_autoToken limits[[\\*]](https://ai.google.dev/gemini-api/docs/tokens)|\n**Input token limit**\n1,048,576\n**Output token limit**\n65,536\n|\nhandymanCapabilities|\n**Audio generation**\nNot supported\n**Batch API**\nSupported\n**Caching**\nSupported\n**Code execution**\nSupported\n**File search**\nSupported\n**Function calling**\nSupported\n**Grounding with Google Maps**\nSupported\n**Image generation**\nNot supported\n**Live API**\nNot supported\n**Search grounding**\nSupported\n**Structured outputs**\nSupported\n**Thinking**\nSupported\n**URL context**\nSupported\n|\n123Versions|\nRead the[model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions)for more details.\n* `Stable: gemini-2.5-pro`\n|\ncalendar\\_monthLatest update|June 2025|\ncognition\\_2Knowledge cutoff|January 2025|\n### Gemini 2.5 Pro TTS\n|Property|Description|\nid\\_cardModel code|`gemini-2.5-pro-preview-tts`|\nsaveSupported data types|\n**Inputs**\nText\n**Output**\nAudio\n|\ntoken\\_autoToken limits[[\\*]](https://ai.google.dev/gemini-api/docs/tokens)|\n**Input token limit**\n8,192\n**Output token limit**\n16,384\n|\nhandymanCapabilities|\n**Audio generation**\nSupported\n**Batch API**\nSupported\n**Caching**\nNot supported\n**Code execution**\nNot supported\n**File search**\nNot Supported\n**Function calling**\nNot supported\n**Grounding with Google Maps**\nNot supported\n**Image generation**\nNot supported\n**Live API**\nNot supported\n**Search grounding**\nNot supported\n**Structured outputs**\nNot supported\n**Thinking**\nNot supported\n**URL context**\nNot supported\n|\n123Versions|\nRead the[model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions)for more details.\n* `gemini-2.5-pro-preview-tts`\n|\ncalendar\\_monthLatest update|December 2025|\n## Previous Gemini models\nOUR SECOND GENERATION WORKHORSE MODEL\n## Gemini 2.0 Flash\nOur second generation workhorse model, with a 1 million token context window.\n### Expand to learn more\nGemini 2.0 Flash delivers next-gen features and improved capabilities,\nincluding superior speed, native tool use, and a 1M token\ncontext window.\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash)\n#### Model details\n### Gemini 2.0 Flash\n|Property|Description|\nid\\_cardModel code|`gemini-2.0-flash`|\nsaveSupported data types|\n**Inputs**\nAudio, images, video, and text\n**Output**\nText\n|\ntoken\\_autoToken limits[[\\*]](https://ai.google.dev/gemini-api/docs/tokens)|\n**Input token limit**\n1,048,576\n**Output token limit**\n8,192\n|\nhandymanCapabilities|\n**Audio generation**\nNot supported\n**Batch API**\nSupported\n**Caching**\nSupported\n**Code execution**\nSupported\n**File search**\nNot supported\n**Function calling**\nSupported\n**Grounding with Google Maps**\nSupported\n**Image generation**\nNot supported\n**Live API**\nNot supported\n**Search grounding**\nSupported\n**Structured outputs**\nSupported\n**Thinking**\nExperimental\n**URL context**\nNot supported\n|\n123Versions|\nRead the[model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions)for more details.\n* Latest:`gemini-2.0-flash`\n* Stable:`gemini-2.0-flash-001`\n* Experimental:`gemini-2.0-flash-exp`\n|\ncalendar\\_monthLatest update|February 2025|\ncognition\\_2Knowledge cutoff|August 2024|\n### Gemini 2.0 Flash Image\n|Property|Description|\nid\\_cardModel code|`gemini-2.0-flash-preview-image-generation`|\nsaveSupported data types|\n**Inputs**\nAudio, images, video, and text\n**Output**\nText and images\n|\ntoken\\_autoToken limits[[\\*]](https://ai.google.dev/gemini-api/docs/tokens)|\n**Input token limit**\n32,768\n**Output token limit**\n8,192\n|\nhandymanCapabilities|\n**Audio generation**\nNot supported\n**Batch API**\nSupported\n**Caching**\nSupported\n**Code execution**\nNot Supported\n**File search**\nNot supported\n**Function calling**\nNot supported\n**Grounding with Google Maps**\nNot supported\n**Image generation**\nSupported\n**Live API**\nNot Supported\n**Search grounding**\nNot Supported\n**Structured outputs**\nSupported\n**Thinking**\nNot Supported\n**URL context**\nNot supported\n|\n123Versions|\nRead the[model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions)for more details.\n* Preview:`gemini-2.0-flash-preview-image-generation`\ngemini-2.0-flash-preview-image-generation is not currently supported in a number of countries in Europe, Middle East &amp; Africa\n|\ncalendar\\_monthLatest update|May 2025|\ncognition\\_2Knowledge cutoff|August 2024|\nOUR SECOND GENERATION FAST MODEL\n## Gemini 2.0 Flash-Lite\nOur second generation small workhorse model, with a 1 million token context window.\n### Expand to learn more\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n#### Model details\n|Property|Description|\nid\\_cardModel code|`gemini-2.0-flash-lite`|\nsaveSupported data types|\n**Inputs**\nAudio, images, video, and text\n**Output**\nText\n|\ntoken\\_autoToken limits[[\\*]](https://ai.google.dev/gemini-api/docs/tokens)|\n**Input token limit**\n1,048,576\n**Output token limit**\n8,192\n|\nhandymanCapabilities|\n**Audio generation**\nNot supported\n**Batch API**\nSupported\n**Caching**\nSupported\n**Code execution**\nNot supported\n**File search**\nNot supported\n**Function calling**\nSupported\n**Grounding with Google Maps**\nNot supported\n**Image generation**\nNot supported\n**Live API**\nNot supported\n**Search grounding**\nNot supported\n**Structured outputs**\nSupported\n**Thinking**\nNot Supported\n**URL context**\nNot supported\n|\n123Versions|\nRead the[model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions)for more details.\n* Latest:`gemini-2.0-flash-lite`\n* Stable:`gemini-2.0-flash-lite-001`\n|\ncalendar\\_monthLatest update|February 2025|\ncognition\\_2Knowledge cutoff|August 2024|\n## Model version name patterns\nGemini models are available in either*stable*,*preview*,*latest*, or*experimental*versions.\n**Note:**The following list refers to the model string naming convention as of\nSeptember, 2025. Models released prior to that may have different naming\nconventions. Refer to the exact model string if you are using an older model.### Stable\nPoints to a specific stable model. Stable models usually don't change. Most\nproduction apps should use a specific stable model.\nFor example:`gemini-2.5-flash`.\n### Preview\nPoints to a preview model which may be used for production. Preview models will\ntypically have billing enabled, might come with more restrictive rate limits and\nwill be deprecated with at least 2 weeks notice.\nFor example:`gemini-2.5-flash-preview-09-2025`.\n### Latest\nPoints to the latest release for a specific model variation. This can be a\nstable, preview or experimental release. This alias will get hot-swapped with\nevery new release of a specific model variation. A**2-week notice**will\nbe provided through email before the version behind latest is changed.\nFor example:`gemini-flash-latest`.\n### Experimental\nPoints to an experimental model which will typically be not be suitable for\nproduction use and come with more restrictive rate limits. We release\nexperimental models to gather feedback and get our latest updates into the hands\nof developers quickly.\nExperimental models are not stable and availability of model endpoints is\nsubject to change.\n## Model deprecations\nFor information about model deprecations, visit the[Gemini deprecations](https://ai.google.dev/gemini-api/docs/deprecations)page.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the[Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the[Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-12-18 UTC.\nNeed to tell us more?[[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-12-18 UTC.\"],[],[]]","image":"https://ai.google.dev/static/site-assets/images/share-gemini-api-2.png","favicon":"https://www.gstatic.com/devrel-devsite/prod/ve08add287a6b4bdf8961ab8a1be50bf551be3816cdd70b7cc934114ff3ad5f10/googledevai/images/favicon-new.png"},{"id":"https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-flash","title":"Gemini 3 Flash | Generative AI on Vertex AI","url":"https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-flash","publishedDate":"2025-12-19T09:54:02.291Z","author":null,"text":"Gemini 3 Flash | Generative AI on Vertex AI | Google Cloud Documentation[Skip to main content](#main-content)\n[![Google Cloud Documentation](https://www.gstatic.com/devrel-devsite/prod/ve08add287a6b4bdf8961ab8a1be50bf551be3816cdd70b7cc934114ff3ad5f10/clouddocs/images/lockup.svg)](https://docs.cloud.google.com/)\n* /\n[Console](https://console.cloud.google.com/)\n* English\n* Deutsch\n* Español –América Latina\n* Français\n* Português –Brasil\n* 中文–简体* 日本語* 한국어Sign in\n[\n![](https://docs.cloud.google.com/_static/clouddocs/images/icons/products/vertex-ai-color.svg)\n](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/overview)\n* [Vertex AI](https://docs.cloud.google.com/vertex-ai/docs)\n* [Generative AI on Vertex AI](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/overview)\n[Start free](https://console.cloud.google.com/freetrial)\n* [Home](https://docs.cloud.google.com/)\n* [Documentation](https://docs.cloud.google.com/docs)\n* [AI and ML](https://docs.cloud.google.com/docs/ai-ml)\n* [Vertex AI](https://docs.cloud.google.com/vertex-ai/docs)\n* [Generative AI on Vertex AI](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/overview)\n* [Guides](https://docs.cloud.google.com/vertex-ai/generative-ai/docs)\nSend feedback# Gemini 3 FlashStay organized with collectionsSave and categorize content based on your preferences.\n**Preview**\nThis product or feature is\nsubject to the \"Pre-GA Offerings Terms\" in the General Service Terms section\nof the[Service Specific\nTerms](https://docs.cloud.google.com/terms/service-terms#1), and the[Additional Terms for Generative AI\nPreview Products](https://cloud.google.com/trustedtester/aitos).\nYou can process personal data for\nthis product or feature\nas outlined in the[Cloud Data Processing\nAddendum](https://docs.cloud.google.com/terms/data-processing-addendum), subject to the obligations and restrictions described in the agreement under\nwhich you access Google Cloud.\nPre-GA products and features are available \"as is\" and might have limited support.\nFor more information, see the[launch stage descriptions](https://cloud.google.com/products/#product-launch-stages).\nGemini 3 Flash combines Gemini 3 Pro&#39;s reasoning capabilities\nwith the Flash line&#39;s levels on latency, efficiency, and cost. It not only\nenables everyday tasks with improved reasoning, but is designed to tackle the\nmost complex agentic workflows.\nGemini 3 Flash uses several new features to improve performance,\ncontrol, and multimodal fidelity:\n* **Thinking level**: Use the`thinking\\_level`parameter to control the amount\nof internal reasoning the model performs (*minimal*,*low*,*medium*, or*high*) to balance response quality, reasoning complexity, latency, and\ncost. The`thinking\\_level`parameter replaces`thinking\\_budget`for\nGemini 3 models.\n**Note:**If you used a thinking budget of`0`with Gemini 2.5 Flash,\nset your thinking level to`MINIMAL`for similar latency and cost; however,\nyou still need to handle thought signatures when using the*minimal*thinking level.\nFor details on the different thinking levels, see[Thinking](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thinking#gemini-3-and-later-models).\n* **Thought signatures**: Stricter validation of[thought signatures](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thought-signatures)improves reliability in multi-turn function calling.\n* **Media resolution**: Use the`media\\_resolution`parameter (*low*,*medium*,*high*, or*ultra high*) to control vision processing for multimodal inputs,\nimpacting token usage and latency. See[Get started with\nGemini 3](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/get-started-with-gemini-3#media_resolution)for default resolution settings.\n* The*ultra high*media resolution level is only available for the`IMAGE`modality.\n* PDF token counts will be listed under the`IMAGE`modality instead of\nthe`DOCUMENT`modality in`usage\\_metadata`.\n* **Multimodal function responses**: Function responses can now include[multimodal objects like images and PDFs in addition to\ntext](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#mm-fr).\n* **Streaming Function calling**:[Stream partial function call arguments](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#streaming-fc)to improve user experience during tool use.\nFor more information on using these features, see[Get started with\nGemini\n3](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/get-started-with-gemini-3).\n[Try inVertex AI](https://console.cloud.google.com/vertex-ai/generative/multimodal/create/text?model=gemini-3-flash-preview)[View inModel Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-3-flash-preview)[(Preview) Deploy example app]()\nNote: To use the \"Deploy example app\" feature, you need a Google Cloud project with billing and Vertex AI API enabled.\n|Model ID|`gemini-3-flash-preview`|\nSupported inputs & outputs|\n* Inputs:\nText,Code,Images,Audio,Video,PDF\n* Outputs:\nText\n|\nToken limits|\n* Maximum input tokens: 1,048,576\n* Maximum output tokens: 65,536 |\nCapabilities|\n* Supported\n* [Grounding with Google Search](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-search)\n* [Code execution](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/code-execution)\n* [System instructions](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction)\n* [Structured output](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output)\n* [Function calling](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling)\n* [Count Tokens](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/get-token-count)\n* [Thinking](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thinking)\n* [Implicit context caching](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview)\n* [Explicit context caching](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview)\n* [Vertex AI RAG Engine](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/rag-overview)\n* [Chat completions](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/migrate/openai/overview)\n* Not supported\n* [Tuning](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/tune-models)\n* [Gemini Live API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api)\n|\nUsage types|\n* Supported\n* [Provisioned Throughput](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput)\n* [Standard PayGo](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/paygo-standard)\n* [Batch prediction](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini)\n* Not supported\n|\nTechnical specifications|\n**Images**photo|\n* Maximum images per prompt:\n900\n* Maximum file size per file for inline data or direct uploads through the console:\n7 MB\n* Maximum file size per file from Google Cloud Storage:\n30 MB\n* Default resolution tokens:\n1120\n* Supported MIME types:\n`image/png`,`image/jpeg`,`image/webp`,`image/heic`,`image/heif`\n|\n**Documents**description|\n* Maximum number of files per prompt:\n900\n* Maximum number of pages per file:\n900\n* Maximum file size per file for the API or Cloud Storage imports:\n50 MB\n* Maximum file size per file for direct uploads through the console:\n7 MB\n* Default resolution tokens:\n560\n* OCR for scanned PDFs:\nNot used by default\n* Supported MIME types:\n`application/pdf`,`text/plain`\n|\n**Video**videocam|\n* Maximum video length (with audio):\nApproximately 45 minutes\n* Maximum video length (without audio):\nApproximately 1 hour\n* Maximum number of videos per prompt:\n10\n* Default resolution tokens per frame:\n70\n* Supported MIME types:\n`video/x-flv`,`video/quicktime`,`video/mpeg`,`video/mpegs`,`video/mpg`,`video/mp4`,`video/webm`,`video/wmv`,`video/3gpp`\n|\n**Audio**mic|\n* Maximum audio length per prompt:\nApproximately 8.4 hours, or up to 1 million tokens\n* Maximum number of audio files per prompt:\n1\n* Speech understanding for:\nAudio summarization, transcription, and translation\n* Supported MIME types:\n`audio/x-aac`,`audio/flac`,`audio/mp3`,`audio/m4a`,`audio/mpeg`,`audio/mpga`,`audio/mp4`,`audio/ogg`,`audio/pcm`,`audio/wav`,`audio/webm`\n|\n**Parameter defaults**tune|\n* Temperature: 0.0-2.0 (default 1.0)\n* topP: 0.0-1.0 (default 0.95)\n* topK: 64 (fixed)\n* candidateCount: 1–8 (default 1)|\nSupported regions|\nModel availability\n(Includes Standard PayGo & Provisioned Throughput)\n|\n* Global\n* global\n|\nSee[Deployments and endpoints](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/locations)for more information.|\nKnowledge cutoff date|January 2025|\nVersions|\n* `gemini-3-flash-preview`\n* Launch stage: Public preview\n* Release date: December 17, 2025|\nSecurity controls|\n**Online prediction**|\n* Data residency\n* CMEK\n* VPC-SC\n* AXT|\n**Batch prediction**|\n* Data residency\n* CMEK\n* VPC-SC\n* AXT|\n**Tuning**|\n* Data residency\n* CMEK\n* VPC-SC\n* AXT|\n**Context caching**|\n* Data residency\n* CMEK\n* VPC-SC\n* AXT|\nSee[Security controls](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/security-controls)for more information.|\nSupported languages|See[Supported languages](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models#expandable-1).|\nPricing|See[Pricing](https://docs.cloud.google.com/vertex-ai/generative-ai/pricing).|\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the[Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the[Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-12-19 UTC.\nNeed to tell us more?[[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Hard to understand\",\"hardToUnderstand\",\"thumb-down\"],[\"Incorrect information or sample code\",\"incorrectInformationOrSampleCode\",\"thumb-down\"],[\"Missing the information/samples I need\",\"missingTheInformationSamplesINeed\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-12-19 UTC.\"],[],[]]","image":"https://docs.cloud.google.com/_static/cloud/images/social-icon-google-cloud-1200-630.png","favicon":"https://www.gstatic.com/devrel-devsite/prod/ve08add287a6b4bdf8961ab8a1be50bf551be3816cdd70b7cc934114ff3ad5f10/clouddocs/images/favicons/onecloud/favicon.ico"},{"id":"https://www.assemblyai.com/pricing","title":"Pricing | Production-ready AI Models - AssemblyAI","url":"https://www.assemblyai.com/pricing","author":null,"text":"AssemblyAI | Pricing | Production-ready AI Models\n[![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67b5bd3d9e8ee1a6b2410b9e_AssemblyAI%20Logo.svg)](https://www.assemblyai.com/)\n# Pricing built for innovation, not contract negotiation\nStart for free, scale seamlessly, and only pay for what you use. AssemblyAI&#x27;s usage-based pricing has no up-front commits or contracts and decreasing rates as you scale.\n## Free\n* Access to industry-leading Speech-to-Text and Audio Intelligence models\n* Transcribe up to 185 hours of pre-recorded audio for free\n* Transcribe up to 333 hours of streaming audio for free\n* Up to 5 new streams per minute\n* Developer docs, community support, and resources to help you build\n[Get started for free](https://www.assemblyai.com/dashboard/signup)\n## Pay as you go\n* Unlimited access to Speech-to-Text, Audio Intelligence, and LeMUR\n* Unlimited concurrent streams and pre-recorded concurrency starting at 200 files\n* Customize rate limits - scale to any workload\n* Dedicated technical support and customized SLAs and SLOs\n* BAA for HIPAA and compliance with EU Data Residency standards\n* Self-hosted deployments (On-prem, EU, VPC)\n[Start building as low as $0.15/hr](https://www.assemblyai.com/dashboard/login)\n## Looking for tiered pricing?\nWhether you&#x27;re an enterprise processing millions of requests looking for tiered pricing options, need dedicated infrastructure, or require custom model configurations, our team will work with you to create a solution fits your specific needs.\n[Talk to our team](https://www.assemblyai.com/contact/sales)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787aff442c1e25786e2c_Logo%20line%20item-9.avif)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787a9f7cff207518cbf5_Logo%20line%20item.avif)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787a40c14e4c346a18a6_Logo%20line%20item-11.avif)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/68d5e9cfbe5c275e91f2846b_Customer%3DCluely%2C%20Color%3DGrey.svg)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787a314db4fd71551d4a_Logo%20line%20item-12.avif)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/68d5e9cf343513e0b16bf6c1_8df259833fc6a229e2278f766dfeaaa8_Customer%3DGranola%2C%20Color%3DGrey.svg)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67f043370e0e42bfbd28e31d_Customer%3DCalabrio%2C%20Color%3DGrey.svg)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/68d6b2a888ff6e636c449d5f_Customer%3DDovetail%2C%20Color%3DWhite-4.avif)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787a3bad32c2d7e80888_4ff28835ad93bb7edfba35c14be2b52f_Logo%20line%20item-10.svg)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787a3ce85bba4381ef97_Logo%20line%20item-5.avif)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787a7b7f78d10038259a_Logo%20line%20item-1.avif)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787a40c14e4c346a17d0_Logo%20line%20item-3.avif)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787a6a3f9c401b23ae15_Logo%20line%20item-6.avif)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787afedb0844d5955c22_Logo%20line%20item-7.avif)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787a3bad32c2d7e80882_Logo%20line%20item-13.avif)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787a421fb4b787bce8e9_Logo%20line%20item-2.avif)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67f0416b232ada6681bf6b90_Customer%3DJotPsych%2C%20Color%3DGrey.svg)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67f0416bf81ec2ea4453db18_Customer%3DDelphi%2C%20Color%3DGrey.svg)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67f043372ae839fa455d4ab6_Customer%3DMetaview%2C%20Color%3DGrey.svg)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787a3bad32c2d7e80888_4ff28835ad93bb7edfba35c14be2b52f_Logo%20line%20item-10.svg)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67b5b36481538ce17bdd0494_runway-logo.svg)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787a33d16b6d8936a2e9_Logo%20line%20item-4.avif)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787adf9230a8bbd6a92c_Logo%20line%20item-8.avif)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787a3ce85bba4381ef97_Logo%20line%20item-5.avif)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67dc787a6a3f9c401b23ae15_Logo%20line%20item-6.avif)\n[\nSpeech-to-Text\n](#pricing_speech-to-text)[\nSpeech Understanding\n](#pricing_speech_understanding)[\nGuardrails\n](#pricing_guardrails)[\nLLM Gateway\n](#pricing_llm-gateway)\n## Pre-recorded Speech-to-Text\nBuild Voice AI on the most accurate Speech-to-Text with built-in speaker diarization, language detection, formatting, filler words, keyterms prompting, custom spelling, word-level timestamps, and more.\n|\nModels\n|\nPay as you go\n|\nCustom\n|\nUniversal\nFast, accurate transcription across 99 languages—exceptional accuracy straight out of the box.\n|\n**$0.15**/hr1\n|\nGet custom rate limits, enhanced concurrency, and enterprise-grade flexibility tailored to your AI workloads\n[\nContact us\n](https://www.assemblyai.com/contact/sales)\n|\nSlam-1\nIdentify a wide range of entities that are spoken in your audio files, such as person and company names, email addresses, dates, and locations.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\nBeta\nHighest accuracy transcription powered by LLM intelligence—understands context, not just words. Only available in English.\n|\n**$0.27**/hr1\n|\n[](#)\n## Streaming Speech-to-Text\nTranscribe live audio and video files in real-time at ultra-low latency and high-quality accuracy. Leverage auto punctuation and casing, next-gen end-of-turn detection, and ITM/formatting.\n|\nModels\n|\nPay as you go\n|\nCustom\n|\nUniversal-Streaming\nUltra-fast, ultra-accurate real-time transcription. Built-in turn detection, and unlimited concurrency.\n|\n**$0.15**/hr1\n|\nGet custom rate limits, enhanced concurrency, and enterprise-grade flexibility tailored to your AI workloads\n[\nContact us\n](https://www.assemblyai.com/contact/sales)\n|\nUniversal-Streaming Multilingual\nIdentify a wide range of entities that are spoken in your audio files, such as person and company names, email addresses, dates, and locations.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\nUltra-fast, ultra-accurate real-time transcription in English, Spanish, French, German, Italian, and Portuguese. Built-in turn detection, and unlimited concurrency.\n|\n**$0.15**/hr1\n|\nKeyterms Prompting\nIdentify a wide range of entities that are spoken in your audio files, such as person and company names, email addresses, dates, and locations.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\nAdd-on\nImprove recognition accuracy for specific words and phrases that are important to your use case.\n|\n**$0.04**/hr\n|\n[](#)\n[](#)\n## Speech Understanding\nAI models to identify speaker names, translate and format outputs, identify spoken topics, and more.\n|\nModels\n|\nPay as you go\n|\nCustom\n|\nSpeaker Identification\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d155bd212f97b00894825a_info-circle.svg)\nSpeaker Identification allows you to identify speakers by their actual names or roles, transforming generic labels like “Speaker A” or “Speaker B” into meaningful identifiers that you provide.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\nNew\n|\n**$0.02**/hr\n|\nGet custom rate limits, enhanced concurrency, and enterprise-grade flexibility tailored to your AI workloads\n[\nContact us\n](https://www.assemblyai.com/contact/sales)\n|\nTranslation\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d155bd212f97b00894825a_info-circle.svg)\nThe Translation feature automatically converts your transcribed audio content from one language to another, enabling you to reach global audiences without manual translation work.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\nNew\n|\n**$0.06**/hr\n|\nCustom Formatting\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d155bd212f97b00894825a_info-circle.svg)\nThe Custom Formatting feature automatically standardizes and formats specific types of information in your transcripts, ensuring consistency across dates, phone numbers, emails, and other data types.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\nNew\n|\n**$0.03**/hr\n|\nEntity Detection\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d155bd212f97b00894825a_info-circle.svg)\nIdentify a wide range of entities that are spoken in your audio files, such as person and company names, email addresses, dates, and locations.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n**$0.08**/hr\n|\nSentiment Analysis\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d155bd212f97b00894825a_info-circle.svg)\nWith Sentiment Analysis, AssemblyAI can detect the sentiment of each sentence of speech spoken in your audio files.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n**$0.02**/hr\n|\nAuto Chapters\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d155bd212f97b00894825a_info-circle.svg)\nAutomatically generate a summary over time for audio and video files.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n**$0.08**/hr\n|\nKey Phrases\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d155bd212f97b00894825a_info-circle.svg)\nAccurately identify significant words and phrases in your transcription, enabling you to extract the most pertinent concepts or highlights from your audio/video file.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n**$0.01**/hr\n|\nTopic Detection\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d155bd212f97b00894825a_info-circle.svg)\nLabel the topics that are spoken in your audio and video files. The predicted topic labels follow the standardized IAB Taxonomy, which makes them suitable for contextual targeting.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n**$0.15**/hr\n|\nSummarization\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d155bd212f97b00894825a_info-circle.svg)\nLeverage our AI-powered Summarization models to automatically summarize audio/video data in your products at scale. Customize the summary types to best fit your use case.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n**$0.03**/hr\n|\n[](#)\n## Guardrails\nGuardrails ensures only high-quality, safe, and compliant content flows through your applications.\n|\nModels\n|\nPay as you go\n|\nCustom\n|\nProfanity Filtering\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d155bd212f97b00894825a_info-circle.svg)\nAutomatically filter out profanity from your transcripts.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n**$0.01**/hr\n|\nGet custom rate limits, enhanced concurrency, and enterprise-grade flexibility tailored to your AI workloads\n[\nContact us\n](https://www.assemblyai.com/contact/sales)\n|\nPII Audio Redaction\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d155bd212f97b00894825a_info-circle.svg)\nIdentify and remove Personally Identifiable Information, such as phone numbers and social security numbers, from the audio file before it is returned to you.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n**$0.05**/hr\n|\nPII Redaction\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d155bd212f97b00894825a_info-circle.svg)\nIdentify and remove Personally Identifiable Information, such as phone numbers and social security numbers, from the transcription text before it is returned to you.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n**$0.08**/hr\n|\nContent Moderation\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d155bd212f97b00894825a_info-circle.svg)\nDetect sensitive content in your audio and video files - such as hate speech, violence, sensitive social issues, alcohol, drugs, and more.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n**$0.15**/hr\n|\n## LLM Gateway\n[The LLM Gateway unifies your voice-to-intelligence workflow into one API.](https://www.assemblyai.com/docs/llm-gateway/apply-llms-to-audio-files)\n|\nModels\n|\nPay as you go\n|\nCustom\n|\nGPT-5\nModel with superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$1.25/ 1m tokens (Input)\n$10.00/ 1m tokens (output)\n|\nGet custom rate limits, enhanced concurrency, and enterprise-grade flexibility tailored to your AI workloads\n[\nContact us\n](https://www.assemblyai.com/contact/sales)\n|\nGPT-5-Mini\nModel with superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$0.25/ 1m tokens (Input)\n$2.00/  1m tokens (output)\n|\nGPT-5 Nano\nModel with superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$0.05/ 1m tokens (Input)\n$0.40/ 1m tokens (output)\n|\nGPT 4.1\nModel with superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$2.00/ 1m tokens (Input)\n$8.00/ 1m tokens (output)\n|\ngpt-oss-20b\nModel with superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$0.07/ 1m tokens (Input)\n$0.30/ 1m tokens (output)\n|\ngpt-oss-120b\nModel with superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$0.15/ 1m tokens (Input)\n$0.60/ 1m tokens (output)\n|\nChatGPT-4o\nModel with superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$5.00/ 1m tokens (Input)\n$15.00/ 1m tokens (output)\n|\nGemini 3 Pro\nModel with superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$2.00/ 1m tokens (Input)\n$12.00/ 1m tokens (output)\n|\nGemini 2.5 Flash\nModel with superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$0.30/ 1m tokens (Input)\n$2.50/ 1m tokens (output)\n|\nGemini 2.5 Pro\nModel with superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$1.25/ 1m tokens (Input)\n$10.00/ 1m tokens (output)\n|\nGemini 2.5 Pro\nModel with superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$1.25/ 1m tokens (Input)\n$10.00/ 1m tokens (output)\n|\nClaude 4.5 Sonnet\nModel with superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$3.00/ 1m tokens (Input)\n$15.00/ 1m tokens (output)\n|\nClaude 4.5 Haiku\nA legacy model with a balanced combination of performance and speed for efficient, high-throughput tasks.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$1.00/ 1m tokens (Input)\n$5.00/ 1m tokens (output)\n|\nClaude 4 Sonnet\nModel with superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$3.00/ 1m tokens (Input)\n$15.00/ 1m tokens (output)\n|\nClaude 4 Opus\nModel with superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$15.00/ 1m tokens (Input)\n$75.00/ 1m tokens (output)\n|\nClaude 3.5 Haiku\nModel with superior performance on complex reasoning tasks, advanced creative work, and sophisticated problem-solving.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$0.80/ 1m tokens (Input)\n$4.00/ 1m tokens (output)\n|\nClaude 3 Haiku\nThe fastest model in the family, optimized for quick responses while maintaining good reasoning.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d15d0f34a3905e96e0d759_arrow-container.svg)\n|\n$0.25/ 1m tokens (Input)\n$1.25/ 1m tokens (output)\n|\n[](#)\n## Security and Privacy\nAssemblyAI uses enterprise-grade security practices to keep your data safe. We approach security by design and default, and continuously ensure AssemblyAI is secure for you and your team.\n![GDPR Compliant](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/68700ab87628629cfd13ce10_ad786dc11a42430d703a4bf6c193ba2a_GDPR-compliant-badge.svg)\nGDPR\n![PCI DSS Compliant](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/68700ac4426f9ecd86a30543_7441d3b136dc812742aea596d5225984_pci-dss-compliant-logo-vector.svg)\nPCI DSS\n![AICPA SOC 2 Compliant](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/68700ab846ec415bf04092df_560564dfacd9f547b7e11a359edf9b2b_soc2_badge.ac7ad1ad.svg)\nSOC 2 Type 2\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/687035bf4748945eebcd8481_Data%20Residency.svg)\nEU Data Residency\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/68702eae536405d7e6aa4602_ISO27001.svg)\nISO 27001\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/6870369a27ff2cf5292166cd_HIPAA.svg)\nHIPAA Compliance\n## Frequently Asked Questions\nWhat are the differences between Speech-to-Text models?\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d184b7023382187a79966b_chevron-up.svg)\nUniversal is a high-accuracy model supporting[99 languages](https://www.assemblyai.com/docs/speech-to-text/pre-recorded-audio/supported-languages), built for general-purpose use cases. It offers strong out-of-the-box performance and supports features like speaker diarization and real-time streaming. Slam-1 is our most advanced speech language model, designed specifically for speech tasks. It uses a prompt-based architecture for deeper contextual understanding and allows domain-specific customization—no retraining needed. Perfect for legal, medical, and other specialized use cases. Universal-Streaming is an ultra-fast, ultra-accurate streaming speech-to-text model designed for voice agents.\nCan I sign up for free?\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d184b7023382187a79966b_chevron-up.svg)\nYes! With the free offer, you get $50 in credits to use towards AssemblyAI’s Speech-to-Text APIs. To add more credits, simply add a credit card to your account.\nDo you offer volume discounts?\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d184b7023382187a79966b_chevron-up.svg)\nAbsolutely! If you plan to send large volumes of audio and video content through our API, please reach out to us[here](https://www.assemblyai.com/contact/sales)to see if you qualify for a volume discount.\nHow does Universal-Streaming concurrency work?\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d184b7023382187a79966b_chevron-up.svg)\nWe don&#x27;t limit how many streams you can run simultaneously - only how quickly you can start new ones, giving you unlimited scale while ensuring reliable performance.\n‍Free users can start 5 new streams per minute, while**pay-as-you-go**accounts start with 100 new streams per minute. Anytime you are using 70% or more of your current limit, your new sessions rate limit will automatically increase and scale up by 10% every 60 seconds. This means within 5 minutes of sustained usage, you can scale from 100 to 146 new streams per minute (for a total of 610 concurrent streams), with unlimited ceiling as your usage grows.\nThese limits are designed to never interfere with legitimate applications - normal scaling patterns automatically get more capacity before hitting any walls, while protecting against runaway scripts or abuse. Your baseline limit is guaranteed and never decreases, so you can scale smoothly from dozens to thousands of simultaneous streams without artificial barriers or surprise fees.\nNeed higher limits?[Contact our sales team](https://www.assemblyai.com/contact/sales)for custom limits that match your deployment timeline.\nHow does Universal-Streaming session-based pricing work?\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d184b7023382187a79966b_chevron-up.svg)\nWe charge based on total session duration - the entire time your connection stays open, whether audio is flowing or not. This gives you complete transparency and control: you pay for exactly what you&#x27;re using, with no hidden costs for idle streams. You can choose to keep streams open continuously for instant response or open them strategically as needed to minimize costs, scaling up and down without prepaid commitments based on how your voice application actually works.\nHow fast does it take for audio and video files to process?\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d184b7023382187a79966b_chevron-up.svg)\nMost audio files sent to AssemblyAI&#x27;s API can be processed in less than 60 seconds. For example, you ca process a 30 minute pre-recorded audio file in 23 seconds with Universal speech-to-text model.\nHow does billing work?\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d184b7023382187a79966b_chevron-up.svg)\nGreat question. Once you add a credit card and deposit funds into your account, your account&#x27;s funds will be drained as you use the API.\nHow is multichannel billed?\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d184b7023382187a79966b_chevron-up.svg)\nWhen multichannel is enabled, each channel will be transcribed and billed separately. The total cost is calculated by taking the hourly transcription rate (billed per second) and multiplying it by the number of channels. To calculate your total cost, simply multiply your recording&#x27;s duration by the hourly rate, then multiply that result by the number of channels.\nFor example, if you sent a 5-minute recording with three channels, you would be billed for the 5 minutes of audio multiplied by the standard rate, with that total multiplied by three channels. This is equivalent to being billed for 15 minutes of audio.\nCan I purchase or use AssemblyAI through the AWS Marketplace?\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d184b7023382187a79966b_chevron-up.svg)\nYou can also get started with[AssemblyAI on the AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-j45hta6jdej7c)—or ask your AWS account team about how to leverage AssemblyAI to revolutionize the way your company understands its customers.\nHow can I talk to someone?\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d184b7023382187a79966b_chevron-up.svg)\nFeel free to email us at[support@assemblyai.com](mailto:support@assemblyai.com), or click the chat button in the bottom right corner of your browser to chat live with our API Support team!\nWhat languages do you support?\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d184b7023382187a79966b_chevron-up.svg)\nWe support over 99 languages and counting, including Global English (English and all of its accents).\nWhat is a token?\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67d184b7023382187a79966b_chevron-up.svg)\nIn the context of a Large Language Model (LLM), a “token” is the smallest unit of text processed by the model. 100 tokens roughly maps to \\~75 words.\n1The rates shown above are offered subject to participation in our[**‍model improvement program**](https://www.assemblyai.com/docs/faq/how-to-opt-out-of-data-sharing-for-our-model-improvement-program)**‍**to help us continue to provide best-in-class speech-to-text.\n## Unlock the value of voice data\nBuild what’s next on the platform powering thousands of the industry’s leading of Voice AI apps.\n[Try our API for free](https://www.assemblyai.com/dashboard/signup)[Contact sales](https://www.assemblyai.com/contact/sales)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67b6012d75b9fd772c9c26fe_home-cta-image.svg)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67e124c2e653ef1ce8ed34ea_shapes.svg)![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67a620d7981ab6847d8dc80c_Noise%20Texture.webp)","image":"https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67e5a4788594c44228a2ed1a_AAI.com%20-%20Meta%20image.avif","favicon":"https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67e119e1e653ef1ce8e17a50_aai-favicon-32x32.png"},{"id":"https://blog.google/technology/developers/build-with-gemini-3-flash/","title":"Build with Gemini 3 Flash: frontier intelligence that scales with you","url":"https://blog.google/technology/developers/build-with-gemini-3-flash/","publishedDate":"2025-12-17T16:00:00.000Z","author":null,"text":"Build with Gemini 3 Flash: frontier intelligence that scales with you\n[Skip to main content](#jump-content)\n[\nThe Keyword\n](https://blog.google/)\nBuild with Gemini 3 Flash, frontier intelligence that scales with you\nShare\n[\nx.com\n]()[\nFacebook\n]()[\nLinkedIn\n]()[\nMail\n]()\nCopy link\nGlobal (English)Africa (English)Australia (English)Brasil (Português)Canada (English)Canada (Français)Česko (Čeština)Deutschland (Deutsch)España (Español)France (Français)India (English)Indonesia (Bahasa Indonesia)Italia (Italiano)日本 (日本語)대한민국 (한국어)Latinoamérica (Español)الشرق الأوسطوشمالأفريقيا(اللغة العربية)MENA (English)Nederlands (Nederland)New Zealand (English)Polska (Polski)Portugal (Português)ประเทศไทย (ไทย)Türkiye (Türkçe)台灣 (中文)\n[&quot;How does Gemini work in Google Maps?&quot;, &quot;What is quantum computing?&quot;, &quot;What are the camera features on Pixel 10?&quot;]\n[Subscribe](https://blog.google/newsletter-subscribe/)\n# Build with Gemini 3 Flash, frontier intelligence that scales with you\nDec 17, 2025\n·Share\n[\nx.com\n]()[\nFacebook\n]()[\nLinkedIn\n]()[\nMail\n]()\nCopy link\nGemini 3 Flash is rolling out to developers now. Learn more about this smarter, scale-ready model and how —and where —you can use it now.\n[![logan-headshot](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/logan-headshot.max-244x184.format-webp.webp)\nLogan Kilpatrick\nGroup Product Manager, Google DeepMind\n](https://blog.google/authors/logan-kilpatrick/)\nRead AI-generated summary\n## General summary\nGoogle is releasing Gemini 3 Flash, a fast and cost-effective model with advanced visual and spatial reasoning. You can use it for coding, gaming, deepfake detection, and document analysis. Access Gemini 3 Flash through Google AI Studio, Antigravity, Gemini CLI, Android Studio, and Vertex AI to improve your projects.\nSummaries were generated by Google AI. Generative AI is experimental.\n## Bullet points\n* \"Build with Gemini 3 Flash\" introduces Google's fast, cost-effective AI model with frontier intelligence.\n* Gemini 3 Flash offers powerful performance at less than a quarter of the cost of Gemini 3 Pro.\n* This new model excels in coding, gaming, deepfake detection, and document analysis.\n* You can access Gemini 3 Flash through Google AI Studio, Antigravity, Vertex AI, and more.\n* Early customers are excited about Gemini 3 Flash's speed and capabilities across various applications.\nSummaries were generated by Google AI. Generative AI is experimental.\n#### Explore other styles:\n* General summary\n* Bullet points\nShare\n[\nx.com\n]()[\nFacebook\n]()[\nLinkedIn\n]()[\nMail\n]()\nCopy link\n![Gemini 3 Flash](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini-3-Flash_developer_header-s.width-200.format-webp.webp)\nYour browser does not support the audio element.\nListen to article\nThis content is generated by Google AI. Generative AI is experimental\n[[duration]] minutes\nVoiceSpeed\nVoice\nSpeed0.75X1X1.5X2X\nToday we’re introducing[Gemini 3 Flash](https://blog.google/products/gemini/gemini-3-flash), our latest model with frontier intelligence built for speed at a fraction of the cost. Building on 3 Pro’s strong multimodal, coding and agentic features, 3 Flash offers powerful performance at less than a quarter the cost of 3 Pro, along with higher rate limits. The new 3 Flash model surpasses 2.5 Pro across many benchmarks while delivering faster speeds. It also features our most[advanced visual and spatial reasoning](https://blog.google/technology/developers/gemini-3-pro-vision/)and now offers[code execution](https://ai.google.dev/gemini-api/docs/code-execution#images)to zoom, count and edit visual inputs.\nFlash remains our most popular version, with 2 and 2.5 Flash processing trillions of tokens across hundreds of thousands of apps built by millions of developers. Our Flash models are truly built for developers, and with 3 Flash, you no longer need to compromise between speed and intelligence.\nGemini 3 Flash is rolling out to developers in the Gemini API via[Google AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-3-flash-preview&amp;utm_source=the_keyword&amp;utm_medium=blog&amp;utm_campaign=g3flash-q4-25),[Google Antigravity](https://antigravity.google/blog/gemini-3-flash-in-google-antigravity),[Gemini CLI](https://developers.googleblog.com/gemini-3-flash-is-now-available-in-gemini-cli/),[Android Studio](https://android-developers.googleblog.com/2025/12/build-smarter-apps-with-gemini-3-flash.html)and to enterprises via[Vertex AI](https://cloud.google.com/blog/products/ai-machine-learning/gemini-3-flash-for-enterprises).\n## Smarter, faster and ready for production at scale\nGemini 3 Flash delivers frontier-class performance on PhD-level reasoning and knowledge benchmarks like GPQA Diamond (90.4%) and Humanity’s Last Exam (33.7% without tools), rivaling much larger frontier models.\nGemini 3 Flash delivers frontier intelligence across top benchmarks.\n![Gemini 3 Flash delivers frontier intelligence across top benchmarks.](https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini-3-flash_final_benchmark-table_light_25-12-17_final.gif)\nGemini 3 Flash is highly efficient without sacrificing intelligence, pushing the Pareto Frontier of performance and efficiency. It outperforms 2.5 Pro while being 3x faster (based on[Artificial Analysis](https://artificialanalysis.ai/models/gemini-3-flash-reasoning)benchmarking) at a fraction of the cost. Even with the lowest[thinking level](https://ai.google.dev/gemini-api/docs/thinking#thinking-levels), 3 Flash often outperforms previous versions with “high” thinking levels.\nGemini 3 Flash pushes the Pareto frontier on performance vs. cost and speed.\nPerformance, here, is measured by[LMArena](https://lmarena.ai/)Elo Score.\n![Gemini 3 Flash pushes the Pareto frontier on performance vs. cost and speed.](https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini-3-flash_pareto-graph_dec17_final.gif)\nIn the Gemini API and Vertex AI, Gemini 3 Flash is priced at $0.50/1M input tokens and $3/1M output tokens (audio input remains at $1/1M input tokens). It comes standard with context caching, allowing for 90% cost reductions in cases with repeated token use over certain thresholds. Similarly, 3 Flash is also available today with the[Batch API](https://ai.google.dev/gemini-api/docs/batch-api), allowing for 50% cost savings and much higher rate limits for asynchronous processing. For synchronous and near real-time use cases, Paid API customers also have access to production-ready rate limits.\n## Gemini 3 Flash in action\nGemini 3 Flash is now integrated into many of our products and early customers are enthusiastic about it. With each new Flash model, it’s exciting to see what new use cases are created.\n**For coding**\nGemini 3 Flash has even better coding and agent capabilities over previous versions, enabling rapid, iterative development, outperforming 3 Pro's agentic coding skill (78% on SWE-bench Verified) but operating faster for quick iterations. Today, 3 Flash is rolling out to users in[Google Antigravity](https://antigravity.google/blog/gemini-3-flash-in-google-antigravity), our new agentic development platform, to provide intelligent coding assistance that keeps pace with your train of thought.\n**For gaming**\nGemini 3 Flash introduces powerful performance for game developers, offering superior video analysis and near real-time reasoning that outperforms previous 2.5 versions.\nAstrocade is using 3 Flash for its agentic game creation engine, generating full game plans and executable code from a single prompt, quickly turning concepts into playable games. (Sequences shortened.)\nBeyond development, Gemini 3 Flash transforms the player experience. It allows[Latitude's](http://latitude.io/)game creation engine to generate smarter characters and more realistic worlds, directly elevating gameplay.\n**For deepfake detection**\n[Resemble AI](http://resemble.ai/)is using Gemini 3 Flash to provide near real-time deepfake intelligence by instantly transforming complex forensic data into simple explanations. They discovered that 3 Flash offered 4x faster multimodal analysis compared to 2.5 Pro, processing raw technical outputs without hindering crucial workflows. You can learn more about this in their[case study](https://www.ai.google.dev/showcase/resembleai).\nResemble AI analyzes a viral deepfake, offering near real-time analysis on why the content is fabricated. (Sequences shortened.)\n**For document analysis**\nPerformance gains often come with a latency tradeoff, but Gemini 3 Flash proves that fast models can still handle the rigorous accuracy demands of the legal industry. With strong reasoning capabilities without sacrificing speed, it enables new levels of efficiency for complex document analysis for[Harvey](http://harvey.ai/), an AI company for law firms and professional service providers.\n## Get started with Gemini 3 Flash\nGemini 3 Flash is available across many of our products, APIs and throughout the ecosystem. As you explore the Gemini 3 family, you have the option to use our new[built-in API logs visualization dashboard](https://aistudio.google.com/logs)and send us model feedback directly through Google AI Studio. Additionally, since 3 Flash is a reasoning model, you will also need to make sure to[circulate thoughts in the API](https://ai.google.dev/gemini-api/docs/thought-signatures)or use the new[Interactions API](https://ai.google.dev/gemini-api/docs/interactions).\nHere’s where you can access Gemini 3 Flash now:\n* [Google AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-3-flash-preview?utm_source=the_keyword&amp;utm_medium=blog&amp;utm_campaign=g3flash-q4-25)and the[Gemini API](http://ai.google.dev/gemini-api)\n* [Google Antigravity](http://antigravity.google/)\n* [Gemini CLI](https://developers.googleblog.com/gemini-3-flash-is-now-available-in-gemini-cli/)\n* [Android Studio](https://android-developers.googleblog.com/2025/12/build-smarter-apps-with-gemini-3-flash.html)\n* [Gemini Code Assist](https://developers.google.com/gemini-code-assist/docs/gemini-3)\n* [Vertex AI](http://console.cloud.google.com/freetrial?redirectPath=/vertex-ai/studio/multimodal)\nWe are excited to put this model in your hands and see what you create with Gemini 3 Flash.\nPOSTED IN:\n### Related stories\n[\n![](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/T5-Keyword_RD1-V01.width-300.format-webp.webp)\nDevelopers#### T5Gemma 2: The next generation of encoder-decoder models\nByBiao Zhang&Ben Hora\nDec 18, 2025\n](https://blog.google/technology/developers/t5gemma-2/)\n[\n![](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/FunctionGemma-Keyword_RD1-V01.width-300.format-webp.webp)\nDevelopers#### FunctionGemma: Bringing bespoke function calling to the edge\nByKat Black&Ravin Kumar\nDec 18, 2025\n](https://blog.google/technology/developers/functiongemma/)\n[\n![](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/kaggle_AI_intensive_hero.width-300.format-webp.webp)\nDevelopers#### Inside Kaggle&#x27;s AI Agents intensive course with Google\nByBrenda Flynn&Anant Nawalgaria\nDec 18, 2025\n](https://blog.google/technology/developers/ai-agents-intensive-recap/)\n[\n![](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Release_Notes_Sundar_Pichai_003.width-300.format-webp.webp)\nAI#### Watch a podcast discussion about Gemini 3 and the future of Search.\nDec 18, 2025\n](https://blog.google/technology/ai/release-notes-podcast-search/)\n[\n![](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemini-3_flash_model_blog_header_.width-300.format-webp.webp)\nGemini Models#### Gemini 3 Flash: frontier intelligence built for speed\nByTulsee Doshi\nDec 17, 2025\n](https://blog.google/products/gemini/gemini-3-flash/)\n[\n![](https://storage.googleapis.com/gweb-uniblog-publish-prod/images/NotesFromGooglePlay_Thumb_16x9.width-300.format-webp.webp)\nGoogle Play#### Look back at Google Play’s biggest launches for developers in 2025.\nDec 16, 2025\n](https://blog.google/feed/year-in-review-developers-2025/)\n.\nJump to position 1Jump to position 2Jump to position 3Jump to position 4Jump to position 5Jump to position 6\n![](https://blog.google/static/blogv2/images/newsletter_toast.svg?version=pr20251215-1743)\nLet’s stay in touch. Get the latest news from Google in your inbox.\n[Subscribe](https://blog.google/newsletter-subscribe/)No thanks","image":"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/FrontierIntelligence_Social.width-1300.png","favicon":"https://blog.google/favicon.ico"},{"id":"https://www.netlify.com/changelog/gemini-3-flash-preview-ai-gateway/","title":"Gemini 3 Flash Preview now available in AI Gateway - Netlify","url":"https://www.netlify.com/changelog/gemini-3-flash-preview-ai-gateway/","publishedDate":"2025-12-17T09:54:02.291Z","author":null,"text":"Gemini 3 Flash Preview now available in AI Gateway\nNew:[Observability, AI Gateway, and Prerendering](https://www.netlify.com/blog/create-deploy-run-ai-across-your-development-workflow/)\nClose announcement bar\nHelp\n* * * * # Gemini 3 Flash Preview now available in AI Gateway\n* [ai gateway](https://www.netlify.com/changelog/tag/ai-gateway/)\nDecember 17, 2025\nGoogle’s Gemini 3 Flash Preview is now available through AI Gateway. You can call this model from Netlify Functions without configuring API keys; the AI Gateway provides the connection to Google for you.\nExample usage in a Function:\n```\n`\nimport{GoogleGenAI}from'@google/genai';\nexportdefaultasync(request:Request,context:Context)=\\>{\nconstai=newGoogleGenAI({});\nconstresponse=awaitai.models.generateContent({\nmodel:'gemini-3-flash-preview',\ncontents:'How does AI work?'\n});\nreturnnewResponse(JSON.stringify({answer:response.text}),{\nheaders:{'Content-Type':'application/json'}\n});\n};\n`\n```\nThis model works across any function type and is compatible with other Netlify primitives such as caching and rate limiting, giving you control over request behavior across your site.\nSee the[AI Gateway documentation](https://docs.netlify.com/build/ai-gateway/overview/)for details.\nAsk Netlify","image":"https://www.netlify.com/images/og-image-default.png","favicon":"https://www.netlify.com/favicon/favicon.ico"},{"id":"https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/get-started-with-gemini-3","title":"Get started with Gemini 3","url":"https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/get-started-with-gemini-3","publishedDate":"2025-11-18T00:00:00.000Z","author":"","text":"Get started with Gemini 3 | Generative AI on Vertex AI | Google Cloud Documentation[Skip to main content](#main-content)\n[![Google Cloud Documentation](https://www.gstatic.com/devrel-devsite/prod/ve08add287a6b4bdf8961ab8a1be50bf551be3816cdd70b7cc934114ff3ad5f10/clouddocs/images/lockup.svg)](https://docs.cloud.google.com/)\n* /\n[Console](https://console.cloud.google.com/)\n* English\n* Deutsch\n* Español\n* Español –América Latina\n* Français\n* Indonesia\n* Italiano\n* Português\n* Português –Brasil\n* 中文–简体* 中文–繁體* 日本語* 한국어Sign in\n[\n![](https://docs.cloud.google.com/_static/clouddocs/images/icons/products/vertex-ai-color.svg)\n](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/overview)\n* [Vertex AI](https://docs.cloud.google.com/vertex-ai/docs)\n* [Generative AI on Vertex AI](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/overview)\n[Start free](https://console.cloud.google.com/freetrial)\n* [Home](https://docs.cloud.google.com/)\n* [Documentation](https://docs.cloud.google.com/docs)\n* [AI and ML](https://docs.cloud.google.com/docs/ai-ml)\n* [Vertex AI](https://docs.cloud.google.com/vertex-ai/docs)\n* [Generative AI on Vertex AI](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/overview)\n* [Guides](https://docs.cloud.google.com/vertex-ai/generative-ai/docs)\nSend feedback# Get started with Gemini 3Stay organized with collectionsSave and categorize content based on your preferences.\n**Preview**\nThis product or feature is\nsubject to the \"Pre-GA Offerings Terms\" in the General Service Terms section\nof the[Service Specific\nTerms](https://docs.cloud.google.com/terms/service-terms#1), and the[Additional Terms for Generative AI\nPreview Products](https://cloud.google.com/trustedtester/aitos).\nYou can process personal data for\nthis product or feature\nas outlined in the[Cloud Data Processing\nAddendum](https://docs.cloud.google.com/terms/data-processing-addendum), subject to the obligations and restrictions described in the agreement under\nwhich you access Google Cloud.\nPre-GA products and features are available \"as is\" and might have limited support.\nFor more information, see the[launch stage descriptions](https://cloud.google.com/products/#product-launch-stages).\nTo see an example of Gemini 3 Pro,\nrun the \"Intro to Gemini 3 Pro\" notebook in one of the following\nenvironments:\n[![](https://docs.cloud.google.com/static/vertex-ai/images/colab-logo-32px.png)Open in Colab](https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_3_pro.ipynb)|[![](https://docs.cloud.google.com/static/vertex-ai/images/colab-enterprise-logo-32px.png)Open in Colab Enterprise](https://console.cloud.google.com/vertex-ai/colab/import/https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_gemini_3_pro.ipynb)|[![](https://docs.cloud.google.com/static/vertex-ai/images/vertex-ai-workbench-logo-32px.png)Open\nin Vertex AI Workbench](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_gemini_3_pro.ipynb)|[![](https://docs.cloud.google.com/static/vertex-ai/images/github-logo-32px.png)View on GitHub](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_3_pro.ipynb)\n**Gemini 3**is our most intelligent model family to date, built\non a foundation of state-of-the-art reasoning. It is designed to bring any\nidea to life by mastering agentic workflows, autonomous coding, and complex\nmultimodal tasks.\nThis guide provides a consolidated, practical path to get started with\nGemini 3 on Vertex AI, highlighting Gemini 3&#39;s key\nfeatures and best practices.\n## Quickstart\nBefore you begin, you must authenticate to Vertex AI using an API\nkey or application default credentials (ADC). See[authentication methods](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/quickstart?usertype=adc)for more information.\n#### Install the Google Gen AI SDK\nGemini 3 API features require Gen AI SDK for Python version 1.51.0 or later.\n```\n`pipinstall--upgradegoogle-genai`\n```\nSet environment variables to use the Gen AI SDK with\nVertex AI\nReplace the`GOOGLE\\_CLOUD\\_PROJECT`value with your Google Cloud Project ID.\nThe Gemini 3 Pro Preview model`gemini-3-pro-preview`and\nGemini 3 Flash Preview model`gemini-3-flash-preview`are only available\non**global**endpoints:\n```\n`exportGOOGLE\\_CLOUD\\_PROJECT=GOOGLE\\_CLOUD\\_PROJECTexportGOOGLE\\_CLOUD\\_LOCATION=globalexportGOOGLE\\_GENAI\\_USE\\_VERTEXAI=True`\n```\n#### Make your first request\nBy default, Gemini 3 Pro and Gemini 3 Flash use dynamic thinking to reason through\nprompts. For faster, lower-latency responses when complex reasoning isn&#39;t\nrequired, you can constrain the model&#39;s`thinking\\_level`. Low thinking is\nideal for high-throughput tasks where speed is paramount.\nFor fast, low-latency responses:\n```\n`fromgoogleimportgenaifromgoogle.genaiimporttypesclient=genai.Client()response=client.models.generate\\_content(model=\"gemini-3-pro-preview\",contents=\"How does AI work?\",config=types.GenerateContentConfig(thinking\\_config=types.ThinkingConfig(thinking\\_level=types.ThinkingLevel.LOW# For fast and low latency response)),)print(response.text)`\n```\n#### Try complex reasoning tasks\nGemini 3 excels at advanced reasoning. For complex tasks like\nmulti-step planning, verified code generation, or deep tool use, use high\nthinking levels. Use these configurations for tasks that previously required\nspecialized reasoning models.\nFor slower, high-reasoning tasks:\n```\n`fromgoogleimportgenaifromgoogle.genaiimporttypesclient=genai.Client()prompt=\"\"\"You are tasked with implementing the classic Thread-Safe Double-Checked Locking (DCL) Singleton pattern in modern C++. This task is non-trivial and requires specialized concurrency knowledge to prevent memory reordering issues.Write a complete, runnable C++ program named `dcl\\_singleton.cpp` that defines a class `Singleton` with a private constructor and a static `getInstance()` method.Your solution MUST adhere to the following strict constraints:1. The Singleton instance pointer (`static Singleton\\*`) must be wrapped in `std::atomic` to correctly manage memory visibility across threads.2. The `getInstance()` method must use `std::memory\\_order\\_acquire` when reading the instance pointer in the outer check.3. The instance creation and write-back must use `std::memory\\_order\\_release` when writing to the atomic pointer.4. A standard `std::mutex` must be used only to protect the critical section (the actual instantiation).5. The `main` function must demonstrate safe, concurrent access by launching at least three threads, each calling `Singleton::getInstance()`, and printing the address of the returned instance to prove all threads received the same object.\"\"\"response=client.models.generate\\_content(model=\"gemini-3-pro-preview\",contents=prompt,config=types.GenerateContentConfig(thinking\\_config=types.ThinkingConfig(thinking\\_level=types.ThinkingLevel.HIGH# Dynamic thinking for high reasoning tasks)),)print(response.text)`\n```\n#### Other thinking levels\nGemini 3 Flash introduces two new thinking levels:`MINIMAL`and`MEDIUM`to give you even more control over how the model handles complex\nreasoning tasks.\n`MINIMAL`offers close-to-zero thinking budget options for tasks optimized for\nthroughput, rather than reasoning.`MEDIUM`allows for a balance between speed\nand reasoning that allows for some reasoning capability but still prioritizes\nlow-latency operations.\n## New API features\nGemini 3 introduces powerful API enhancements and new parameters\ndesigned to give developers**granular control over performance (latency,\ncost), model behavior, and multimodal fidelity**.\nThis table summarizes the core new features and parameters available, along\nwith direct links to their detailed documentation:\n|New Feature/API Change|Documentation|\nModel:`gemini-3-pro-preview`|[Model card](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-pro)[Model Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-3-pro)|\nThinking level|[Thinking](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thinking)|\nMedia resolution|[Image understanding](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/image-understanding)[Video understanding](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/video-understanding)[Audio understanding](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/audio-understanding)[Document understanding](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/document-understanding)|\nThought signature|[Thought signatures](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thought-signatures)|\nTemperature|[API reference](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#generationconfig)|\nMultimodal function responses|[Function calling: Multimodal function responses](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#mm-fr)|\nStreaming function calling|[Function calling: Streaming function calling](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#streaming-fc)|\n### Thinking level\nThe`thinking\\_level`parameter lets you specify a thinking budget for\nthe model&#39;s response generation. By selecting one of two states, you can\nexplicitly balance the trade-offs between response quality and reasoning\ncomplexity and latency and cost.\n* `MINIMAL`:**(Gemini 3 Flash only)**Constrains the model to\nuse as few tokens as possible for thinking and is best used for\nlow-complexity tasks that wouldn&#39;t benefit from extensive reasoning.`MINIMAL`is as close as possible to a zero budget for thinking, but still\nrequires[thought signatures](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thought-signatures).\n* `LOW`: Constrains the model to use fewer tokens for thinking and is suitable\nfor simpler tasks where extensive reasoning is not required.`LOW`is ideal\nfor high-throughput tasks where speed is essential.\n* `MEDIUM`:**(Gemini 3 Flash only)**Offers a balanced approach\nsuitable for tasks of moderate complexity that benefit from reasoning but\ndon&#39;t require deep, multi-step planning. It provides more reasoning\ncapability than`LOW`while maintaining lower latency than`HIGH`.\n* `HIGH`: Allows the model to use more tokens for thinking and is suitable for\ncomplex prompts requiring deep reasoning, such as multi-step planning,\nverified code generation, or advanced function calling scenarios. This is\nthe default level for Gemini 3 Pro and Gemini 3 Flash. Use\nthis configuration when replacing tasks you might have previously relied on\nspecialized reasoning models for.**Note:**You cannot use both`thinking\\_level`and the legacy`thinking\\_budget`parameter in the same request. Doing so will return a`400`error. The`thinking\\_level`parameter is optional. If omitted, the model will use its\ndefault setting (`HIGH`).#### Gen AI SDK example\n```\n`fromgoogleimportgenaifromgoogle.genaiimporttypesclient=genai.Client()response=client.models.generate\\_content(model=\"gemini-3-pro-preview\",contents=\"Find the race condition in this multi-threaded C++ snippet: [code here]\",config=types.GenerateContentConfig(thinking\\_config=types.ThinkingConfig(thinking\\_level=types.ThinkingLevel.HIGH# Default, dynamic thinking)),)print(response.text)`\n```\n#### OpenAI compatibility example\nFor users utilizing the OpenAI compatibility layer, standard parameters are\nautomatically mapped to Gemini 3 equivalents:\n* `reasoning\\_effort`maps to`thinking\\_level`.\n* `none`: maps to`thinking\\_level`minimal (Gemini 3 Flash only).\n* `medium`: maps to`thinking\\_level`medium for Gemini 3 Flash, and`thinking\\_level`high for Gemini 3 Pro.\n```\n`importopenaifromgoogle.authimportdefaultfromgoogle.auth.transport.requestsimportRequestcredentials,\\_=default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])client=openai.OpenAI(base\\_url=f\"https://aiplatform.googleapis.com/v1/projects/{PROJECT\\_ID}/locations/global/endpoints/openapi\",api\\_key=credentials.token,)prompt=\"\"\"Write a bash script that takes a matrix represented as a string withformat '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\"\"\"response=client.chat.completions.create(model=\"gemini-3-pro-preview\",reasoning\\_effort=\"medium\",# Map to thinking\\_level high.messages=[{\"role\":\"user\",\"content\":prompt}],)print(response.choices[0].message.content)`\n```\n### Media resolution\nGemini 3 introduces granular control over multimodal vision\nprocessing using the`media\\_resolution`parameter. Higher resolutions improve\nthe model&#39;s ability to read fine text or identify small details, but increase\ntoken usage and latency. The`media\\_resolution`parameter determines the**maximum number of tokens allocated per input image, PDF page or video\nframe.**\nYou can set the resolution to`low`,`medium`, or`high`globally\n(using`generation\\_config`) or for individual media parts. The`ultra\\_high`resolution can only be set for individual media parts. If unspecified, the\nmodel uses optimal defaults based on the media type.\n#### Token counts\nThis table summarizes the approximate token counts for each`media\\_resolution`value and media type.\n|Media Resolution|Image|Video|PDF|\n`UNSPECIFIED`(Default)|1120|70|560|\n`LOW`|280|70|280 + Text|\n`MEDIUM`|560|70|560 + Text|\n`HIGH`|1120|280|1120 + Text|\n`ULTRA\\_HIGH`|2240|N/A|N/A|\n#### Recommended settings\n|Media Resolution|Max Tokens|Usage Guidance|\n`ultra\\_high`|2240|Tasks requiring analysis of fine details in images, such as processing screen recording stills or high-resolution photos.|\n`high`|1120|Image analysis tasks to ensure maximum quality.|\n`medium`|560||\n`low`|Image: 280 Video: 70|Sufficient for most tasks.**Note:**For Video`low`is a maximum of 70 tokens per frame.|\n**Note:****Token counts for multimodal inputs (images, video. Audio, PDF)**are\nan estimation based on the chosen`media\\_resolution`. As such, the result\nfrom the`count\\_tokens`API call may not match the final consumed tokens. The\naccurate usage for billing is only available after execution within the\nresponse&#39;s`usage\\_metadata`. Because`media\\_resolution`directly impacts\ntoken count, you may need to lower the resolution (for example, to`LOW`) to fit\nvery long inputs, such as long videos or extensive documents.#### Setting`media\\_resolution`per individual part\nYou can set`media\\_resolution`per individual media part:\n```\n`fromgoogleimportgenaifromgoogle.genaiimporttypesclient=genai.Client()response=client.models.generate\\_content(model=\"gemini-3-pro-preview\",contents=[types.Part(file\\_data=types.FileData(file\\_uri=\"gs://cloud-samples-data/generative-ai/image/a-man-and-a-dog.png\",mime\\_type=\"image/jpeg\",),media\\_resolution=types.PartMediaResolution(level=types.PartMediaResolutionLevel.MEDIA\\_RESOLUTION\\_HIGH# High resolution),),Part(file\\_data=types.FileData(file\\_uri=\"gs://cloud-samples-data/generative-ai/video/behind\\_the\\_scenes\\_pixel.mp4\",mime\\_type=\"video/mp4\",),media\\_resolution=types.PartMediaResolution(level=types.PartMediaResolutionLevel.MEDIA\\_RESOLUTION\\_LOW# Low resolution),),\"When does the image appear in the video? What is the context?\",],)print(response.text)`\n```\n#### Setting`media\\_resolution`globally\nYou can also set`media\\_resolution`globally (using`GenerateContentConfig`):\n```\n`fromgoogleimportgenaifromgoogle.genaiimporttypesclient=genai.Client()response=client.models.generate\\_content(model=\"gemini-3-pro-preview\",contents=[types.Part(file\\_data=types.FileData(file\\_uri=\"gs://cloud-samples-data/generative-ai/image/a-man-and-a-dog.png\",mime\\_type=\"image/jpeg\",),),\"What is in the image?\",],config=types.GenerateContentConfig(media\\_resolution=types.MediaResolution.MEDIA\\_RESOLUTION\\_LOW,# Global setting),)print(response.text)`\n```\n### Thought signatures\nThought signatures are encrypted tokens that preserve the model&#39;s reasoning\nstate during multi-turn conversations, specifically when using function\ncalling.\nWhen a thinking model decides to call an external tool, it pauses its\ninternal reasoning process. The thought signature acts as a &quot;save state,&quot;\nallowing the model to resume its chain of thought seamlessly once you provide\nthe function&#39;s result.\nFor more information, see[Thought signatures](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thought-signatures).\n#### Why are thought signatures important?\nWithout thought signatures, the model &quot;forgets&quot; its specific reasoning steps\nduring the tool execution phase. Passing the signature back ensures:\n* **Context continuity**: The model preserves the reason for why the tool was called.\n* **Complex reasoning**: Enables multi-step tasks where the output of one\ntool informs the reasoning for the next.#### Where are thought signatures returned?\nGemini 3 Pro and Gemini 3 Flash enforce stricter validation and updated handling on\nthought signatures which were originally introduced in Gemini 2.5. To ensure\nthe model maintains context across multiple turns of a conversation, you must\nreturn the thought signatures in your subsequent requests.\n* Model responses with a function call will always return a thought\nsignature, even when using the`MINIMAL`thinking level.\n* When there are parallel function calls, the first function call part\nreturned by the model response will have a thought signature.\n* When there are sequential function calls (multi-step), each function call\nwill have a signature and clients are expected to pass signature back\n* Model responses without a function call will return a thought signature\ninside the last part returned by the model.#### How to handle thought signatures?\nThere are two primary ways to handle thought signatures: automatically using the\nGen AI SDKs or OpenAI API, or manually if you are interacting\nwith the API directly.\n##### Automated handling (recommended)\nIf you are using the Google Gen AI SDKs (Python, Node.js, Go,\nJava) or OpenAI Chat Completions API, and utilizing the standard chat history\nfeatures or appending the full model response,`thought\\_signatures`are\nhandled automatically. You don&#39;t need to make any changes to your code.\n###### Manual function calling example\nWhen using the Gen AI SDK, thought signatures are handled\nautomatically by appending the full model response in sequential model requests:\n```\n`fromgoogleimportgenaifromgoogle.genaiimporttypesclient=genai.Client()# 1. Define your toolget\\_weather\\_declaration=types.FunctionDeclaration(name=\"get\\_weather\",description=\"Gets the current weather temperature for a given location.\",parameters={\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\"}},\"required\":[\"location\"],},)get\\_weather\\_tool=types.Tool(function\\_declarations=[get\\_weather\\_declaration])# 2. Send a message that triggers the toolprompt=\"What's the weather like in London?\"response=client.models.generate\\_content(model=\"gemini-3-pro-preview\",contents=prompt,config=types.GenerateContentConfig(tools=[get\\_weather\\_tool],thinking\\_config=types.ThinkingConfig(include\\_thoughts=True)),)# 4. Handle the function callfunction\\_call=response.function\\_calls[0]location=function\\_call.args[\"location\"]print(f\"Model wants to call:{function\\_call.name}\")# Execute your tool (e.g., call an API)# (This is a mock response for the example)print(f\"Calling external tool for:{location}\")function\\_response\\_data={\"location\":location,\"temperature\":\"30C\",}# 5. Send the tool's result back# Append this turn's messages to history for a final response.# The `content` object automatically attaches the required thought\\_signature behind the scenes.history=[types.Content(role=\"user\",parts=[types.Part(text=prompt)]),response.candidates[0].content,# Signature preserved heretypes.Content(role=\"tool\",parts=[types.Part.from\\_function\\_response(name=function\\_call.name,response=function\\_response\\_data,)],)]response\\_2=client.models.generate\\_content(model=\"gemini-3-pro-preview\",contents=history,config=types.GenerateContentConfig(tools=[get\\_weather\\_tool],thinking\\_config=types.ThinkingConfig(include\\_thoughts=True)),)# 6. Get the final, natural-language answerprint(f\"\\\\nFinal model response:{response\\_2.text}\")`\n```\n###### Automatic function calling example\nWhen using the Gen AI SDK in automatic function calling,\nthought signatures are handled automatically:\n```\n`fromgoogleimportgenaifromgoogle.genaiimporttypesdefget\\_current\\_temperature(location:str)-&gt;dict:\"\"\"Gets the current temperature for a given location.Args:location: The city and state, for example San Francisco, CAReturns:A dictionary containing the temperature and unit.\"\"\"# ... (implementation) ...return{\"temperature\":25,\"unit\":\"Celsius\"}client=genai.Client()response=client.models.generate\\_content(model=\"gemini-3-pro-preview\",contents=\"What's the temperature in Boston?\",config=types.GenerateContentConfig(tools=[get\\_current\\_temperature],))print(response.text)# The SDK handles the function call and thought signature, and returns the final text`\n```\n###### OpenAI compatibility example\nWhen using OpenAI Chat Completions API, thought signatures are handled\nautomatically by appending the full model response in sequential model requests:\n```\n`...# Append user prompt and assistant response including thought signaturesmessages.append(response1.choices[0].message)# Execute the tooltool\\_call\\_1=response1.choices[0].message.tool\\_calls[0]result\\_1=get\\_current\\_temperature(\\*\\*json.loads(tool\\_call\\_1.function.arguments))# Append tool response to messagesmessages.append({\"role\":\"tool\",\"tool\\_call\\_id\":tool\\_call\\_1.id,\"content\":json.dumps(result\\_1),})response2=client.chat.completions.create(model=\"gemini-3-pro-preview\",messages=messages,tools=tools,extra\\_body={\"extra\\_body\":{\"google\":{\"thinking\\_config\":{\"include\\_thoughts\":True,},},},},)print(response2.choices[0].message.tool\\_calls)`\n```\nSee the[full code example](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/chat-completions/intro_chat_completions_api.ipynb).\n##### Manual handling\nIf you are interacting with the API directly or managing raw JSON payloads,\nyou must correctly handle the`thought\\_signature`included in the model&#39;s turn.\nYou**must**return this signature in the exact part where it was received\nwhen sending the conversation history back.\nIf proper signatures are not returned, Gemini 3 will return a 400\nError &quot;`&lt;Function Call&gt;`in the &lt;index of contents array&gt; content block is\nmissing a`thought\\_signature`&quot;.\n### Multimodal function responses\nMultimodal function calling allows users to have function responses containing\nmultimodal objects allowing for improved utilization of function calling\ncapabilities of the model. Standard function calling only supports text-based\nfunction responses:\n```\n`fromgoogleimportgenaifromgoogle.genaiimporttypesclient=genai.Client()# This is a manual, two turn multimodal function calling workflow:# 1. Define the function toolget\\_image\\_declaration=types.FunctionDeclaration(name=\"get\\_image\",description=\"Retrieves the image file reference for a specific order item.\",parameters={\"type\":\"object\",\"properties\":{\"item\\_name\":{\"type\":\"string\",\"description\":\"The name or description of the item ordered (e.g., 'green shirt').\"}},\"required\":[\"item\\_name\"],},)tool\\_config=types.Tool(function\\_declarations=[get\\_image\\_declaration])# 2. Send a message that triggers the toolprompt=\"Show me the green shirt I ordered last month.\"response\\_1=client.models.generate\\_content(model=\"gemini-3-pro-preview\",contents=[prompt],config=types.GenerateContentConfig(tools=[tool\\_config],))# 3. Handle the function callfunction\\_call=response\\_1.function\\_calls[0]requested\\_item=function\\_call.args[\"item\\_name\"]print(f\"Model wants to call:{function\\_call.name}\")# Execute your tool (e.g., call an API)# (This is a mock response for the example)print(f\"Calling external tool for:{requested\\_item}\")function\\_response\\_data={\"image\\_ref\":{\"$ref\":\"dress.jpg\"},}function\\_response\\_multimodal\\_data=types.FunctionResponsePart(file\\_data=types.FunctionResponseFileData(mime\\_type=\"image/png\",display\\_name=\"dress.jpg\",file\\_uri=\"gs://cloud-samples-data/generative-ai/image/dress.jpg\",))# 4. Send the tool's result back# Append this turn's messages to history for a final response.history=[types.Content(role=\"user\",parts=[types.Part(text=prompt)]),response\\_1.candidates[0].content,types.Content(role=\"tool\",parts=[types.Part.from\\_function\\_response(name=function\\_call.name,response=function\\_response\\_data,parts=[function\\_response\\_multimodal\\_data])],)]response\\_2=client.models.generate\\_content(model=\"gemini-3-pro-preview\",contents=history,config=types.GenerateContentConfig(tools=[tool\\_config],thinking\\_config=types.ThinkingConfig(include\\_thoughts=True)),)print(f\"\\\\nFinal model response:{response\\_2.text}\")`\n```\n### Streaming function calling\nYou can use streaming partial function call arguments to improve streaming\nexperience on tool use. This feature can be enabled by explicitly setting`stream\\_function\\_call\\_arguments`to`true`:\n```\n`fromgoogleimportgenaifromgoogle.genaiimporttypesclient=genai.Client()get\\_weather\\_declaration=types.FunctionDeclaration(name=\"get\\_weather\",description=\"Gets the current weather temperature for a given location.\",parameters={\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\"}},\"required\":[\"location\"],},)get\\_weather\\_tool=types.Tool(function\\_declarations=[get\\_weather\\_declaration])forchunkinclient.models.generate\\_content\\_stream(model=\"gemini-3-pro-preview\",contents=\"What's the weather in London and New York?\",config=types.GenerateContentConfig(tools=[get\\_weather\\_tool],tool\\_config=types.ToolConfig(function\\_calling\\_config=types.FunctionCallingConfig(mode=types.FunctionCallingConfigMode.AUTO,stream\\_function\\_call\\_arguments=True,)),),):function\\_call=chunk.function\\_calls[0]iffunction\\_callandfunction\\_call.name:print(f\"{function\\_call.name}\")print(f\"will\\_continue={function\\_call.will\\_continue}\")`\n```\nModel response example:\n```\n`{\"candidates\":[{\"content\":{\"role\":\"model\",\"parts\":[{\"functionCall\":{\"name\":\"get\\_weather\",\"willContinue\":true}}]}}]}`\n```\n### Temperature\n* `Range for Gemini 3: 0.0 - 2.0 (default: 1.0)`\nFor Gemini 3, it is strongly recommended to keep the`temperature`parameter at its default value of**`1.0`.**\nWhile previous models often benefited from tuning temperature to control\ncreativity versus determinism, Gemini 3&#39;s reasoning capabilities\nare optimized for the default setting.\nChanging the temperature (setting it to less than`1.0`) may lead to unexpected\nbehavior, such as looping or degraded performance, particularly in complex\nmathematical or reasoning tasks.\n## Supported features\nGemini 3 models also support the following features:\n* [System instructions](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction)\n* [Structured output](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output)\n* [Function calling](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling)\n* [Grounding with Google Search](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-search)\n* [Code execution](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/code-execution)\n* [URL Context](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/url-context)\n* [Thinking](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thinking)\n* [Context caching](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview)\n* [Count tokens](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/get-token-count)\n* [Chat completions](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/migrate/openai/overview)\n* [Batch prediction](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini)\n* [Provisioned Throughput](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput/overview)\n* [Standard pay-as-you-go](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/standard-paygo)## Prompting best practices\nGemini 3 is a reasoning model, which changes how you should prompt.\n* **Precise instructions:**Be concise in your input prompts. Gemini 3\nresponds best to direct, clear instructions. It may over-analyze verbose\nor overly complex prompt engineering techniques used for older models.\n* **Output verbosity:**By default, Gemini 3 is less verbose and\nprefers providing direct, efficient answers. If your use case requires a\nmore conversational or &quot;chatty&quot; persona, you must explicitly steer the\nmodel in the prompt (for example, &quot;Explain this as a friendly, talkative\nassistant&quot;).\n* **Grounding:**For grounding use cases, we recommend using the following\ndeveloper instructions:`You are a strictly grounded assistant limited to the information provided in the User Context. In your answers, rely \\*\\*only\\*\\* on the facts that are directly mentioned in that context. You must \\*\\*not\\*\\* access or utilize your own knowledge or common sense to answer. Do not assume or infer from the provided facts; simply report them exactly as they appear. Your answer must be factual and fully truthful to the provided text, leaving absolutely no room for speculation or interpretation. Treat the provided context as the absolute limit of truth; any facts or details that are not directly mentioned in the context must be considered \\*\\*completely untruthful\\*\\* and \\*\\*completely unsupported\\*\\*. If the exact answer is not explicitly written in the context, you must state that the information is not available.`\n* **Using the Google Search tool:**When using the Google Search tool,\nGemini 3 Flash can at times confuse the current date / time for\nevents in 2024. This can result in the model\nformulating search queries for the wrong year. To ensure the model utilizes\nthe correct time period, explicitly reinforce the current date in`system instructions`:`For time-sensitive user queries that require up-to-date information, you MUST follow the provided current time (date and year) when formulating search queries in tool calls. Remember it is 2025 this year.`\n* **Knowledge cutoff**: For certain queries, Gemini 3 Flash benefits\nfrom being explicitly told its knowledge cutoff. This is the case when the\nGoogle Search tool is disabled and the query explicitly requires the model\nto be able to identify the cutoff data in parametric knowledge.**Recommendation**: Adding the following clause to`system instructions`:`Your knowledge cutoff date is January 2025.`\n* **Using`media\\_resolution`**: Use the`media\\_resolution`parameter to control the\nmaximum number of tokens the model uses to represent an image or frames in\nvideos. High resolution will enable the model to capture details in an image,\nand may use more tokens per frame, while lower resolution allows for optimizing\ncost and latency for images with less visual detail.\n* **Improving video analysis**: Use a higher Frame Per Second (FPS) sampling rate for\nvideos requiring granular temporal analysis, such as fast-action understanding\nor high-speed motion tracking.## Migration considerations\nConsider the following features and constraints when when migrating:\n* **Thinking level**: Gemini 3 models use the`thinking\\_level`parameter to control the amount of internal reasoning the\nmodel performs (*low*or*high*) and to balance\nresponse quality, reasoning complexity, latency, and cost.\n* **Temperature settings:**If your existing code explicitly sets`temperature`(especially to low values for deterministic outputs), it is\nrecommended to remove this parameter and use the Gemini 3 default\nof`1.0`to avoid potential looping issues or performance degradation on\ncomplex tasks.\n* **Thought signatures**: For Gemini 3 models, if a\nthought signature is expected in a turn but not provided, the model\nreturns an error instead of a warning.\n* **Media resolution and tokenization**: Gemini 3 models\nuse a variable sequence length for media tokenization instead of Pan\nand Scan, and have new default resolutions and token costs for images,\nPDFs, and video.\n* **Token counting for multimodality input:**Token counts for multimodal\ninputs (images, video. audio) are an estimation based on the chosen`media\\_resolution`. As such, the result from the`count\\_tokens`API call\nmay not match the final consumed tokens. The accurate usage for billing is\nonly available after execution within the response&#39;s`usage\\_metadata`.\n* **Token consumption:**Migrating to Gemini 3 defaults may**increase**token usage for images and PDFs but**decrease**token usage for video.\nIf requests now exceed the context window due to higher default\nresolutions, it is recommended to explicitly reduce the media resolution.\n* **PDF &amp; document understanding:**Default OCR resolution for PDFs has\nchanged. If you relied on specific behavior for dense document parsing,\ntest the new`media\\_resolution: &quot;&quot;high&quot;&quot;`setting to ensure continued\naccuracy. For Gemini 3 models, PDF token counts in`usage\\_metadata`are reported under the IMAGE modality instead of DOCUMENT.\n* **Image segmentation:**Image segmentation is not supported by\nGemini 3 models. For workloads requiring built-in image\nsegmentation, it is recommended to continue to utilize Gemini 2.5 Flash\nwith thinking turned off.\n* **Multimodal function responses**: For Gemini 3 models,\nyou can include image and PDF data in function responses.## FAQ\n1. **What is the knowledge cutoff for Gemini 3 Pro?**Gemini 3 has a knowledge cutoff of January 2025.\n2. **Which region is**`gemini-3-pro-preview`**available on Google Cloud?**Global.\n3. **What are the context window limits?**Gemini 3 models support a 1 million token input context window and up to 64k tokens of output.\n4. Does`gemini-3-pro-preview`support image output? No.\n5. Does`gemini-3-pro-preview`support[Gemini Live API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api)? No.\n## What's next\n* Learn more about[Gemini 3 Pro](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-pro).\n* Try the[Intro to Gemini 3 Pro](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_3_pro.ipynb)notebook tutorial.\n* Learn about[Function calling](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling).\n* Learn about[Thinking](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/thinking).\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the[Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the[Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-12-19 UTC.\nNeed to tell us more?[[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Hard to understand\",\"hardToUnderstand\",\"thumb-down\"],[\"Incorrect information or sample code\",\"incorrectInformationOrSampleCode\",\"thumb-down\"],[\"Missing the information/samples I need\",\"missingTheInformationSamplesINeed\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-12-19 UTC.\"],[],[]]","image":"https://docs.cloud.google.com/_static/cloud/images/social-icon-google-cloud-1200-630.png","favicon":"https://www.gstatic.com/devrel-devsite/prod/ve08add287a6b4bdf8961ab8a1be50bf551be3816cdd70b7cc934114ff3ad5f10/clouddocs/images/favicons/onecloud/favicon.ico"},{"id":"https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models","title":"Google models","url":"https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models","publishedDate":"2025-10-17T00:00:00.000Z","author":"","text":"Google models | Generative AI on Vertex AI | Google Cloud Documentation[Skip to main content](#main-content)\n[![Google Cloud Documentation](https://www.gstatic.com/devrel-devsite/prod/ve08add287a6b4bdf8961ab8a1be50bf551be3816cdd70b7cc934114ff3ad5f10/clouddocs/images/lockup.svg)](https://docs.cloud.google.com/)\n* /\n[Console](https://console.cloud.google.com/)\n* English\n* Deutsch\n* Español\n* Español –América Latina\n* Français\n* Indonesia\n* Italiano\n* Português\n* Português –Brasil\n* 中文–简体* 中文–繁體* 日本語* 한국어Sign in\n[\n![](https://docs.cloud.google.com/_static/clouddocs/images/icons/products/vertex-ai-color.svg)\n](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/overview)\n* [Vertex AI](https://docs.cloud.google.com/vertex-ai/docs)\n* [Generative AI on Vertex AI](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/overview)\n[Start free](https://console.cloud.google.com/freetrial)\n* [Home](https://docs.cloud.google.com/)\n* [Documentation](https://docs.cloud.google.com/docs)\n* [AI and ML](https://docs.cloud.google.com/docs/ai-ml)\n* [Vertex AI](https://docs.cloud.google.com/vertex-ai/docs)\n* [Generative AI on Vertex AI](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/overview)\n* [Guides](https://docs.cloud.google.com/vertex-ai/generative-ai/docs)\nSend feedback# Google modelsStay organized with collectionsSave and categorize content based on your preferences.\n## Featured Gemini models\n3 Propreview\nDesigned for comprehensive multimodal understanding and complex problem solving\n* Features a 1 million token context window\n* Excels in agentic workflows and autonomous coding tasks\n* Designed for complex multimodal tasks and advanced reasoning[](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-pro)\n3 Flashpreview\nOur most powerful agentic and coding model, with the best multimodal understanding capabilities\n* The latest in our workhorse line of Gemini models\n* enhanced multimodal and coding capabilities\n* Features our new near-zero thinking level option[](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-flash)\n2.5 Flash Image🍌\nJumpstart your creative workflow with image generation and conversational editing\n* Generate high-quality images\n* Capable of turn-based conversational editing\n* Same balance of speed and price as Gemini 2.5 Flash[](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-image)\n## Generally available Gemini models\ndiamond[Gemini 2.5 Pro](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro)Our high-capability model for complex reasoning and coding. Features adaptive thinking capabilities to solve complex agentic and multimodal challenges with a 1 million token context.\nspark[Gemini 2.5 Flash](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash)Lightning-fast and highly capable. Delivers a balance of intelligence and latency with controllable thinking budgets for versatile applications.\n🍌[Gemini 2.5 Flash Image](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-image)Turn ideas into production-ready assets. Features conversational editing, multi-image fusion, and character consistency for advanced creative workflows.\nperformance\\_auto[Gemini 2.5 Flash-Lite](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-lite)Built for massive scale. Balances cost and performance for high-throughput tasks, optimized for efficiency without sacrificing multimodal understanding.\naudio\\_spark[Gemini 2.5 Flash with Gemini Live API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-live-api)Designed for real-time, bidirectional streaming. Features low-latency built-in audio and affective dialogue capabilities for natural, conversational interactions.\nspark[Gemini 2.0 Flash](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash)Multimodal performance for developers needing a cost-effective model for general-purpose tasks.\nperformance\\_auto[Gemini 2.0 Flash-Lite](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash-lite)Streamlined and ultra-efficient for simple, high-frequency tasks where speed and price are the priority.\n## Preview Gemini models\npreview[Gemini 3 Pro](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-pro)Our latest reasoning-first model optimized for complex agentic workflows and coding. Features adaptive thinking, a 1M token context window, and integrated grounding for sophisticated multimodal problem solving.\npreview[Gemini 3 Flash](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-flash)Our best model for complex multimodal understanding, designed to tackle the most challenging agentic problems with strong coding and state-of-the-art reasoning capabilities.\npreview[Gemini 3 Pro Image](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-pro-image)High-fidelity image generation with reasoning-enhanced composition. Supports legible text rendering, complex multi-turn editing, and character consistency using up to 14 reference inputs.\n## Gemma models\n![](https://ai.google.dev/gemma/images/gemma_sq.png)[Gemma 3n](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3n)An open model designed for efficient execution on low-resource devices, supporting multimodal input (text, image, video, and audio) and text output in over 140 languages.\n![](https://ai.google.dev/gemma/images/gemma_sq.png)[Gemma 3](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3)An open model featuring text and image input, support for over 140 languages, and a 128K context window.\n![](https://ai.google.dev/gemma/images/gemma_sq.png)[Gemma 2](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma2)An open model supporting text generation, summarization, and extraction.\n![](https://ai.google.dev/gemma/images/gemma_sq.png)[Gemma](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335)A small, lightweight open model\nsupporting text generation, summarization, and extraction.\n![](https://ai.google.dev/gemma/images/gemma_sq.png)[ShieldGemma 2](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/shieldgemma2)Instruction-tuned models for evaluating text and image safety against defined policies.\n![](https://ai.google.dev/gemma/images/gemma_sq.png)[PaliGemma](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/paligemma)An open vision-language model combining SigLIP and Gemma.\n![](https://ai.google.dev/gemma/images/gemma_sq.png)[CodeGemma](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/codegemma)A powerful, lightweight open model for coding tasks, including code completion, generation, and understanding.\n![](https://ai.google.dev/gemma/images/gemma_sq.png)[TxGemma](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/txgemma)A model that generates predictions, classifications, or text based on therapeutic-related data, for building AI models with less data and compute.\n![](https://ai.google.dev/gemma/images/gemma_sq.png)[MedGemma](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/medgemma)A collection of Gemma 3 variants trained for performance on medical text and image comprehension.\n![](https://ai.google.dev/gemma/images/gemma_sq.png)[MedSigLIP](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/medsiglip)A SigLIP variant trained to encode medical images and text into a common embedding space.\n![](https://ai.google.dev/gemma/images/gemma_sq.png)[T5Gemma](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/T5Gemma)A family of lightweight encoder-decoder research models.\n## Embeddings models\nwidth\\_normal[Embeddings for Text](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-embedding-001)Converts text data into vector representations for semantic search, classification, and clustering.\nwidth\\_normal[Multimodal Embeddings](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/multimodalembedding)Generates vectors based on images, for tasks such as image classification and search.\n## Imagen models\nphoto\\_spark[Imagen 4 for Generation](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/imagen/4-0-generate#4.0-generate-001)Use text prompts to generate novel images\nwith higher quality than our previous image generation models\nphoto\\_spark[Imagen 4 for Fast Generation](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/imagen/4-0-generate#4.0-fast-generate-001)Use text prompts to generate novel images\nwith higher quality and lower latency than our previous image generation\nmodels\nphoto\\_spark[Imagen 4 for Ultra Generation](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/imagen/4-0-generate#4.0-ultra-generate-001)Use text prompts to generate novel images\nwith higher quality and better prompt adherence than our previous image\ngeneration models\nphoto\\_spark[Imagen 3 for Generation 002](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/imagen/3-0-generate#3.0-generate-002)Use text prompts to generate novel images\nphoto\\_spark[Imagen 3 for Generation 001](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/imagen/3-0-generate#3.0-generate-001)Use text prompts to generate novel images\nphoto\\_spark[Imagen 3 for Fast Generation](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/imagen/3-0-generate#3.0-fast-generate-001)Use text prompts to generate novel images\nwith lower latency than our other image generation models\nimage\\_edit\\_auto[Imagen 3 for Editing and Customization](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/imagen/3-0-generate#3.0-capability-001)Edits existing images or generates new images based on text prompts and provided context.\n## Preview Imagen models\nphoto\\_spark[Virtual Try-On](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/imagen/virtual-try-on-preview-08-04)Generates images of people wearing clothing products.\nimage\\_edit\\_auto[Imagen product recontext on Vertex AI](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/imagen/imagen-product-recontext-preview-06-30)Edits product images to place them in different scenes or backgrounds based on text prompts.\n## Veo models\nmovie[Veo 2 Generate](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/veo/2-0-generate#2.0-generate-001)Generates videos from text prompts and images.\nmovie[Veo 3 Generate](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/veo/3-0-generate#3.0-generate-001)Generates videos from text prompts and images with high quality.\nmovie[Veo 3 Fast](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/veo/3-0-generate#3.0-fast-generate-001)Generates videos from text prompts and images with high quality and low latency.\nmovie[Veo 3.1 Generate](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/veo/3-1-generate#3.1-generate-001)Generates videos from text prompts and images with high quality.\nmovie[Veo 3.1 Fast](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/veo/3-1-generate#3.1-fast-generate-001)Generates videos from text prompts and images with high quality and low latency.\n## Preview Veo models\nmovie[Veo 3 Generate preview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/veo/3-0-generate#3.0-generate-preview)Generates videos from text prompts and images with high quality.\nmovie[Veo 3 Fast preview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/veo/3-0-generate#3.0-fast-generate-preview)Generates videos from text prompts and images with high quality and low latency.\nmovie[Veo 3.1 Generate preview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/veo/3-1-generate#3.1-generate-preview)Generates videos from text prompts and images with high quality.\nmovie[Veo 3.1 Fast preview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/veo/3-1-generate#3.1-fast-generate-preview)Generates videos from text prompts and images with high quality and low latency.\nmovie[Veo 2 Preview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/veo/2-0-generate#2.0-generate-preview)Generates videos from text prompts and images, supporting inpaint and outpaint.\n## Experimental Veo models\nmovie[Veo 2 Experimental](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/veo/2-0-generate#2.0-generate-exp)An experimental model with features under test.\n## MedLM models\n**Caution:**MedLM is deprecated. Access to MedLM will no\nlonger be available on or after September 29, 2025.\nmedical\\_information[MedLM-medium](https://docs.cloud.google.com/static/vertex-ai/generative-ai/docs/medlm/MedLM-model-card.pdf)A HIPAA-compliant model for medical question answering and summarization of healthcare documents.\nclinical\\_notes[MedLM-large-large](https://docs.cloud.google.com/static/vertex-ai/generative-ai/docs/medlm/MedLM-model-card.pdf)A HIPAA-compliant model for medical question answering and summarization of healthcare documents.\n## Language support\n### Gemini\nAll the Gemini models can understand and respond in the\nfollowing languages:\nAfrikaans (`af`),\nAlbanian (`sq`),\nAmharic (`am`),\nArabic (`ar`),\nArmenian (`hy`),\nAssamese (`as`),\nAzerbaijani (`az`),\nBasque (`eu`),\nBelarusian (`be`),\nBengali (`bn`),\nBosnian (`bs`),\nBulgarian (`bg`),\nCatalan (`ca`),\nCebuano (`ceb`),\nChinese (Simplified and Traditional) (`zh`),\nCorsican (`co`),\nCroatian (`hr`),\nCzech (`cs`),\nDanish (`da`),\nDhivehi (`dv`),\nDutch (`nl`),\nEnglish (`en`),\nEsperanto (`eo`),\nEstonian (`et`),\nFilipino (Tagalog) (`fil`),\nFinnish (`fi`),\nFrench (`fr`),\nFrisian (`fy`),\nGalician (`gl`),\nGeorgian (`ka`),\nGerman (`de`),\nGreek (`el`),\nGujarati (`gu`),\nHaitian Creole (`ht`),\nHausa (`ha`),\nHawaiian (`haw`),\nHebrew (`iw`),\nHindi (`hi`),\nHmong (`hmn`),\nHungarian (`hu`),\nIcelandic (`is`),\nIgbo (`ig`),\nIndonesian (`id`),\nIrish (`ga`),\nItalian (`it`),\nJapanese (`ja`),\nJavanese (`jv`),\nKannada (`kn`),\nKazakh (`kk`),\nKhmer (`km`),\nKorean (`ko`),\nKrio (`kri`),\nKurdish (`ku`),\nKyrgyz (`ky`),\nLao (`lo`),\nLatin (`la`),\nLatvian (`lv`),\nLithuanian (`lt`),\nLuxembourgish (`lb`),\nMacedonian (`mk`),\nMalagasy (`mg`),\nMalay (`ms`),\nMalayalam (`ml`),\nMaltese (`mt`),\nMaori (`mi`),\nMarathi (`mr`),\nMeiteilon (Manipuri) (`mni-Mtei`),\nMongolian (`mn`),\nMyanmar (Burmese) (`my`),\nNepali (`ne`),\nNorwegian (`no`),\nNyanja (Chichewa) (`ny`),\nOdia (Oriya) (`or`),\nPashto (`ps`),\nPersian (`fa`),\nPolish (`pl`),\nPortuguese (`pt`),\nPunjabi (`pa`),\nRomanian (`ro`),\nRussian (`ru`),\nSamoan (`sm`),\nScots Gaelic (`gd`),\nSerbian (`sr`),\nSesotho (`st`),\nShona (`sn`),\nSindhi (`sd`),\nSinhala (Sinhalese) (`si`),\nSlovak (`sk`),\nSlovenian (`sl`),\nSomali (`so`),\nSpanish (`es`),\nSundanese (`su`),\nSwahili (`sw`),\nSwedish (`sv`),\nTajik (`tg`),\nTamil (`ta`),\nTelugu (`te`),\nThai (`th`),\nTurkish (`tr`),\nUkrainian (`uk`),\nUrdu (`ur`),\nUyghur (`ug`),\nUzbek (`uz`),\nVietnamese (`vi`),\nWelsh (`cy`),\nXhosa (`xh`),\nYiddish (`yi`),\nYoruba (`yo`),\nand Zulu (`zu`).\n### Gemma\nGemma and Gemma 2 support only the English (`en`) language. Gemma 3 and Gemma 3n provide multilingual support in over 140 languages.\n### Embeddings\nMultilingual text embedding models support the following languages:\nAfrikaans (`af`),\nAlbanian (`sq`),\nAmharic (`am`),\nArabic (`ar`),\nArmenian (`hy`),\nAzerbaijani (`az`),\nBasque (`eu`),\nBelarusian (`be`),\nBengali (`bn`),\nBulgarian (`bg`),\nCatalan (`ca`),\nCebuano (`ceb`),\nChinese (Simplified and Traditional) (`zh`),\nCorsican (`co`),\nCzech (`cs`),\nDanish (`da`),\nDutch (`nl`),\nEnglish (`en`),\nEsperanto (`eo`),\nEstonian (`et`),\nFilipino (Tagalog) (`fil`),\nFinnish (`fi`),\nFrench (`fr`),\nFrisian (`fy`),\nGalician (`gl`),\nGeorgian (`ka`),\nGerman (`de`),\nGreek (`el`),\nGujarati (`gu`),\nHaitian Creole (`ht`),\nHausa (`ha`),\nHawaiian (`haw`),\nHebrew (`iw`),\nHindi (`hi`),\nHmong (`hmn`),\nHungarian (`hu`),\nIcelandic (`is`),\nIgbo (`ig`),\nIndonesian (`id`),\nIrish (`ga`),\nItalian (`it`),\nJapanese (`ja`),\nJavanese (`jv`),\nKannada (`kn`),\nKazakh (`kk`),\nKhmer (`km`),\nKorean (`ko`),\nKurdish (`ku`),\nKyrgyz (`ky`),\nLao (`lo`),\nLatin (`la`),\nLatvian (`lv`),\nLithuanian (`lt`),\nLuxembourgish (`lb`),\nMacedonian (`mk`),\nMalagasy (`mg`),\nMalay (`ms`),\nMalayalam (`ml`),\nMaltese (`mt`),\nMaori (`mi`),\nMarathi (`mr`),\nMongolian (`mn`),\nMyanmar (Burmese) (`my`),\nNepali (`ne`),\nNyanja (Chichewa) (`ny`),\nNorwegian (`no`),\nPashto (`ps`),\nPersian (`fa`),\nPolish (`pl`),\nPortuguese (`pt`),\nPunjabi (`pa`),\nRomanian (`ro`),\nRussian (`ru`),\nSamoan (`sm`),\nScots Gaelic (`gd`),\nSerbian (`sr`),\nSesotho (`st`),\nShona (`sn`),\nSindhi (`sd`),\nSinhala (Sinhalese) (`si`),\nSlovak (`sk`),\nSlovenian (`sl`),\nSomali (`so`),\nSpanish (`es`),\nSundanese (`su`),\nSwahili (`sw`),\nSwedish (`sv`),\nTajik (`tg`),\nTamil (`ta`),\nTelugu (`te`),\nThai (`th`),\nTurkish (`tr`),\nUkrainian (`uk`),\nUrdu (`ur`),\nUzbek (`uz`),\nVietnamese (`vi`),\nWelsh (`cy`),\nXhosa (`xh`),\nYiddish (`yi`),\nYoruba (`yo`),\nand Zulu (`zu`).\n### Imagen 3\nImagen3 supports the following languages:\nEnglish (`en`),\nChinese (Simplified and Traditional) (`zh`),\nHindi (`hi`),\nJapanese (`ja`),\nKorean (`ko`),\nPortuguese (`pt`),\nand Spanish (`es`).\n### MedLM\nThe MedLM model supports the\nEnglish (`en`) language.\n## Explore all models in Model Garden\nModel Garden is a platform that helps you discover, test, customize,\nand deploy Google proprietary and select OSS models and assets. To explore\nthe generative AI models and APIs that are available on Vertex AI, go to\nModel Garden in the Google Cloud console.\n[Go to Model Garden](https://console.cloud.google.com/vertex-ai/model-garden)\nTo learn more about Model Garden, including available models and\ncapabilities, see[Explore AI models in Model Garden](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models).\n## Model versions\nTo see all model versions, including legacy and retired models, see[Model versions and lifecycle](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions).\n## What's next\n* Try a quickstart tutorial using[Vertex AI Studio](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/quickstarts/quickstart)or\nthe[Vertex AI API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/quickstarts/quickstart-multimodal).\n* Explore pretrained models in[Model Garden](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models).\n* Learn how to control access to specific models in Model Garden by\nusing a[Model Garden organization\npolicy](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/control-model-access).\n* Learn about[pricing](https://cloud.google.com/vertex-ai/generative-ai/pricing).\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the[Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the[Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the[Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-12-19 UTC.\nNeed to tell us more?[[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Hard to understand\",\"hardToUnderstand\",\"thumb-down\"],[\"Incorrect information or sample code\",\"incorrectInformationOrSampleCode\",\"thumb-down\"],[\"Missing the information/samples I need\",\"missingTheInformationSamplesINeed\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-12-19 UTC.\"],[],[]]","image":"https://docs.cloud.google.com/_static/cloud/images/social-icon-google-cloud-1200-630.png","favicon":"https://www.gstatic.com/devrel-devsite/prod/ve08add287a6b4bdf8961ab8a1be50bf551be3816cdd70b7cc934114ff3ad5f10/clouddocs/images/favicons/onecloud/favicon.ico"},{"id":"https://assemblyai.com/blog/one-platform-multiple-models-simplifying-voice-ai-llm-gateway","title":"One platform, multiple models: Simplifying Voice AI with LLM Gateway","url":"https://assemblyai.com/blog/one-platform-multiple-models-simplifying-voice-ai-llm-gateway","publishedDate":"2025-10-27T00:00:00.000Z","author":null,"text":"One platform, multiple models: Simplifying Voice AI with LLM Gateway\n[![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67b5bd3d9e8ee1a6b2410b9e_AssemblyAI%20Logo.svg)](https://assemblyai.com/)\n[![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67a0a359c2339010b16a54f7_circle-icon.svg)\nReleases &amp; Updates\n](https://assemblyai.com/collection/releases)\nOctober 27, 2025\n# One platform, multiple models: Simplifying Voice AI with LLM Gateway\nLLM Gateway unifies voice AI development with one API for transcription and LLM processing. Access OpenAI, Anthropic, and Google models without managing multiple integrations.\nKelsey Foster\n,\nGrowth\n[LLMs](https://assemblyai.com/topic/llms)\n[LLM Gateway](https://assemblyai.com/topic/llm-gateway)\nReviewed by\nNo items found.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67a0fb99445797bcd265154a_16x9%20cover%20full.webp)![](https://cdn.prod.website-files.com/67a1e6de2f2eab2e125f8b9a/68ff7ef061961fa81e329d42_Blog-LLM%20Gateway.png)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67a2034bfd4b7a0c6135ef75_Noise.webp)\nTable of contents\n[[Visible on live site]](#)\n[![Embedded YouTube video](https://img.youtube.com/vi/jBqja2wmZLU/0.jpg)](https://www.youtube.com/watch?v=jBqja2wmZLU)\nVoice AI teams face a common frustration: building a single feature requires orchestrating multiple vendors, managing separate API keys, and maintaining custom integration code. You&#x27;re transcribing audio with one service, sending that transcript to an LLM provider, and wrangling the response back into your application. It works, but it&#x27;s fragile, time-consuming, and expensive to maintain.\nA medical documentation platform processes thousands of patient visits daily. Their engineering team maintains integrations with a speech-to-text provider, routes transcripts to multiple LLM endpoints depending on the task, and manages error handling across this entire chain. When a new model launches or an API schema changes, they&#x27;re back in the code, updating pipelines and testing edge cases.\nThis fragmentation isn&#x27;t just a technical inconvenience, but can be a strategic bottleneck. Teams spend weeks building orchestration layers instead of improving their core product. They&#x27;re locked into specific providers because switching means rewriting integration code. And when something breaks, debugging across multiple services turns a five-minute fix into an afternoon of vendor coordination.\nLLM Gateway changes this. It&#x27;s a unified platform that takes you from raw audio to structured insights through a single API, with access to leading language models from OpenAI, Anthropic, and Google.\n## **The hidden cost of fragmented voice AI stacks**\nMost voice AI applications follow a predictable pattern: capture audio, transcribe it, process the transcript with an LLM, and extract insights. Simple in theory. In practice, it means managing relationships with multiple vendors, each with their own authentication, billing, rate limits, and quirks.\nA call center analytics company processing 400,000+ minutes of audio monthly described their workflow: transcribe with one API, copy the transcript to another service for sentiment analysis, route it elsewhere for summarization, then parse everything back into a consistent format their application can use. Each step introduces latency, potential failures, and maintenance overhead.\nThese costs compound over time. New models release frequently: GPT-4.5, Claude Opus 4, Gemini 2.0. But accessing them means waiting for your LLM provider to add support, updating your integration code, and testing thoroughly before deploying. Teams often wait weeks between a model&#x27;s announcement and actually using it in production.\nThen there&#x27;s the operational burden. Engineering teams juggle multiple API keys, navigate different authentication patterns, reconcile separate billing statements, and debug issues that span vendor boundaries. A transcription error? Check the speech-to-text logs. LLM output not formatted correctly? Different support channel, different troubleshooting process.\n## **How LLM Gateway works**\nLLM Gateway provides a single API endpoint that handles the entire flow from audio to insights. You send audio files or configure streaming transcription, specify which LLM you want to use, and receive structured results, all through AssemblyAI&#x27;s infrastructure.\n### **From fragmented to unified**\nThe platform integrates deeply with transcript data. Instead of copying text between services, you prompt directly against transcripts within the same API call. This tight integration enables context-aware processing. For example, you can specify that a transcript is from a &quot;Native American history lecture&quot; or a &quot;medical consultation,&quot; and the LLM uses that context to generate more accurate outputs.\n### **Multi-model flexibility**\nLLM Gateway provides access to multiple frontier models through a consistent interface:\n* **OpenAI&#x27;s GPT family**: GPT-4o, GPT-4o mini, GPT-4 Turbo\n* **Anthropic&#x27;s Claude**: Claude Opus 4, Claude Sonnet 4, Claude Haiku\n* **Google&#x27;s Gemini**: Gemini 2.0 Flash, Gemini 1.5 Pro, Gemini 1.5 Flash\nThese models support tool calling, streaming responses, and internet search capabilities. The schema remains consistent across providers, so switching from GPT to Claude means changing one parameter, not rewriting integration code.\nThis flexibility enables optimization strategies that were previously too complex to implement. Use Claude Haiku for extracting simple metadata (fast and cheap), Claude Sonnet for detailed summaries (balanced performance), and Claude Opus 4 for complex reasoning tasks (maximum capability). You&#x27;re not locked into a single model&#x27;s pricing or performance characteristics.\n## **Real-world applications**\n### **Medical documentation automation**\nHealthcare providers spend[20-30% of their time on documentation](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6371396/). One telehealth platform automated this using LLM Gateway&#x27;s integrated processing.\nTheir workflow: patient consultation audio streams to AssemblyAI&#x27;s real-time transcription, the completed transcript is immediately processed by Claude to generate structured SOAP notes (Subjective, Objective, Assessment, Plan), and formatted documentation appears in their system within 30 seconds of the call ending.\nThe key improvement wasn&#x27;t just automation, but consistency. The LLM Gateway processes every consultation with the same level of detail and structure, regardless of the provider&#x27;s documentation style. Clinicians review and approve notes rather than writing them from scratch, reducing documentation time by 60-70%.\n### **Call center intelligence**\nA call center analytics company processing over 50,000 hours of customer conversations monthly needed to extract trends, sentiment, and compliance risks at scale. Their previous approach involved three separate services: speech-to-text, sentiment analysis, and keyword extraction, each requiring its own integration.\nWith LLM Gateway, they consolidated everything. Real-time transcription captures conversations as they happen, GPT-4 analyzes sentiment and flags compliance concerns, and structured data flows directly into their reporting dashboard. When they want to test Claude&#x27;s performance on sentiment detection versus GPT&#x27;s, they change one parameter without needing to rewrite code.\nThe operational impact was immediate. Instead of debugging issues across three vendor relationships, they work with one support team. When GPT-4.5 launched, they tested it in production within 24 hours. Their engineering team stopped maintaining orchestration code and started building features that differentiate their product.\n### **Legal intake optimization**\nLaw firms handling hundreds of intake calls daily needed to score agent performance, identify high-value cases, and ensure regulatory compliance. The technical challenge: processing streaming audio, generating real-time scorecards, and translating conversations in a HIPAA-compliant manner.\nTheir solution combined AssemblyAI&#x27;s streaming transcription with LLM Gateway&#x27;s model selection. For real-time agent feedback during calls, they use GPT-4o mini (low latency, cost-effective). For post-call analysis and case summaries, they switch to Claude Sonnet (deeper reasoning, better at legal context). The entire stack runs through one API, one set of credentials, one compliance framework.\n## **Technical implementation**\nGetting started requires minimal changes to existing AssemblyAI integrations. If you&#x27;re already transcribing audio, you add LLM processing by including a prompt in your API request.\n### **Basic example**\nHere&#x27;s a simple implementation that transcribes audio and generates a summary:\n```\n`importrequestsimporttime# Step 1: Transcribe the audiobase\\_url =&quot;https://api.assemblyai.com&quot;headers = {&quot;authorization&quot;:&quot;&quot;&lt;&lt;YOUR\\_API\\_KEY&gt;&gt;&quot;&quot;}# You can use a local filepath:# with open(&quot;./my-audio.mp3&quot;, &quot;rb&quot;) as f:# response = requests.post(base\\_url + &quot;&quot;/v2/upload&quot;&quot;,# headers=headers,# data=f)# upload\\_url = response.json()[&quot;&quot;upload\\_url&quot;&quot;]# Or use a publicly-accessible URL:upload\\_url =&quot;&quot;https://assembly.ai/sports\\_injuries.mp3&quot;&quot;data = {&quot;&quot;audio\\_url&quot;&quot;: upload\\_url}response = requests.post(base\\_url +&quot;/v2/transcript&quot;, headers=headers, json=data)transcript\\_id = response.json()[&quot;id&quot;]polling\\_endpoint = base\\_url +f&quot;/v2/transcript/{transcript\\_id}&quot;whileTrue:transcript = requests.get(polling\\_endpoint, headers=headers).json()iftranscript[&quot;status&quot;] ==&quot;completed&quot;:breakeliftranscript[&quot;status&quot;] ==&quot;error&quot;:raiseRuntimeError(f&quot;Transcription failed:{transcript[&#x27;error&#x27;]}&quot;)else:time.sleep(3)# Step 2: Send transcript to LLM Gatewayprompt =&quot;Provide a brief summary of the transcript.&quot;llm\\_gateway\\_data = {&quot;model&quot;:&quot;claude-sonnet-4-5-20250929&quot;,&quot;messages&quot;: [{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:f&quot;{prompt}\\\\n\\\\nTranscript:{transcript[&#x27;text&#x27;]}&quot;}],&quot;&quot;max\\_tokens&quot;&quot;:1000}response = requests.post(&quot;https://llm-gateway.assemblyai.com/v1/chat/completions&quot;,headers=headers,json=llm\\_gateway\\_data)print(response.json()[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;])`\n```\nYou&#x27;ll see this output after running the code above:\n```\n`The transcript describes several common sports injuries - runner&#x27;s knee,sprained ankle, meniscus tear, rotator cuff tear, and ACL tear. It providesdefinitions, causes, and symptoms for each injury. The transcript seems to benarrating sports footage and describing injuries as they occur to the athletes.Overall, it provides an overview of these common sports injuries that can resultfrom overuse or sudden trauma during athletic activities`\n```\n‍### **Model selection strategies**\nDifferent tasks benefit from different models. Here&#x27;s a practical framework:\n**Use GPT-4o mini or Claude Haiku for:**\n* Extracting simple metadata (speaker names, dates, phone numbers)\n* Classification tasks (routing, categorization)\n* High-volume processing where speed and cost matter\n**Use GPT-4o or Claude Sonnet for:**\n* Detailed summaries requiring understanding of context\n* Sentiment analysis with nuanced interpretation\n* Multi-step reasoning over transcript content\n**Use Claude Opus 4 or GPT-4 Turbo for:**\n* Complex legal or medical document generation\n* Deep analysis requiring extensive context\n* Tasks where output quality justifies higher cost## **Pricing and availability**\nLLM Gateway uses pass-through pricing for language model usage. You pay the same rates you&#x27;d pay going directly to OpenAI, Anthropic, or Google, plus AssemblyAI&#x27;s transcription costs. We don&#x27;t mark up LLM usage or require minimum commitments, and billing consolidates through your existing AssemblyAI account.\nThe platform is available now. Existing AssemblyAI customers can start using LLM Gateway by adding the configuration to their API requests. New users can sign up and access both transcription and LLM processing through the same account.\n## **Why consolidation matters**\nConsolidated AI infrastructure isn&#x27;t just about convenience, it&#x27;s about velocity. Teams building voice AI features compete on how quickly they can ship improvements, test new models, and respond to user feedback. When your infrastructure spans multiple vendors, each iteration cycle gets slower.\nLLM Gateway removes that friction. New models become available within a day of release because AssemblyAI maintains the integrations. Schema changes and API updates happen transparently. When you want to A/B test Claude versus GPT on summarization quality, you change one parameter and compare results—no separate integration work needed.\nThis consolidation also improves reliability. Instead of debugging failures across multiple services (&quot;Was it the transcription? The LLM? The network between them?&quot;), you work with a single provider who owns the entire flow. When something breaks, you file one support ticket, not three.\nFor developers, it means writing less plumbing code and building more features. For product teams, it means faster experimentation and iteration. For companies, it means predictable costs and reduced operational complexity.\nVoice AI development requires speed and flexibility. Teams that experiment quickly and adopt new models rapidly gain competitive advantage. LLM Gateway provides that foundation by unifying transcription and LLM processing, so you can focus on building differentiated features instead of managing integrations.\nUnify Top LLMs with One API\nUse AssemblyAI's LLM Gateway to generate summaries, extract action items, and power Q&A over your audio data—without juggling multiple provider integrations.\n[Sign up now](https://www.assemblyai.com/dashboard/signup)\nTitle goes here\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\n[Button Text](#)\n#### Related posts\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67a0fb99445797bcd265154a_16x9%20cover%20full.webp)![](https://cdn.prod.website-files.com/67a1e6de2f2eab2e125f8b9a/68f0e33d024801babf71f093_Blog-7%20LLM%20use%20cases%20and%20applications%20in%202024.png)\nDecember 1, 2025\n### 7 LLM use cases and applications in 2026\n[](https://assemblyai.com/blog/llm-use-cases)\nBy\nKelsey Foster\n,\nGrowth\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67a0fb99445797bcd265154a_16x9%20cover%20full.webp)![](https://cdn.prod.website-files.com/67a1e6de2f2eab2e125f8b9a/691f52eb4004edcab594bbc5_Blog-Gemini%203%20Pro%20vs%20GPT-5%20vs%20Claude%204.5%20-%20Which%20Model%20Wins%20for%20Audio%20Workflows.png)\nNovember 20, 2025\n### Gemini 3 Pro vs GPT-5 vs Claude 4.5: Which model wins for audio workflows?\n[](https://assemblyai.com/blog/gemini-3-pro-vs-gpt-5-vs-claude-4-5)\nBy\nMeredith Rauch\n,\nGrowth\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67a0fb99445797bcd265154a_16x9%20cover%20full.webp)![](https://cdn.prod.website-files.com/67a1e6de2f2eab2e125f8b9a/6900e13f6f9bc6c40cfc8b4b_Blog-Extract%20phone%20call%20insights%20with%20LLMs%20in%20Python.png)\nOctober 28, 2025\n### Extract phone call insights with LLMs in Python\n[](https://assemblyai.com/blog/extract-call-insights-llms-python)\nBy\nRyan O&#x27;Connor\n,\nSenior Developer Educator\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67a0fb99445797bcd265154a_16x9%20cover%20full.webp)![](https://cdn.prod.website-files.com/67a1e6de2f2eab2e125f8b9a/68ff7ef061961fa81e329d42_Blog-LLM%20Gateway.png)\nOctober 27, 2025\n### One platform, multiple models: Simplifying Voice AI with LLM Gateway\n[](https://assemblyai.com/blog/one-platform-multiple-models-simplifying-voice-ai-llm-gateway)\nBy\nKelsey Foster\n,\nGrowth\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67a0fb99445797bcd265154a_16x9%20cover%20full.webp)![](https://cdn.prod.website-files.com/67a1e6de2f2eab2e125f8b9a/67c61ffce65dae9f8941aed9_AR-Fireflies.avif)\nDecember 4, 2024\n### Market timing, partnering with the right AI providers, and building your own competitive moat\n[](https://assemblyai.com/blog/assemblyai-fireflies)\nBy\n,\nNo items found.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67a0fb99445797bcd265154a_16x9%20cover%20full.webp)![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67a0fb99445797bcd265154a_16x9%20cover%20full.webp)\nJanuary 5, 2024\n### AssemblyAI&#x27;s New Integrations &amp; Latest Tutorials\n[](https://assemblyai.com/blog/assemblyai-newsletter-17)\nBy\nSmitha Kolan\n,\nDeveloper Educator\nNo items found.\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67a0fb99445797bcd265154a_16x9%20cover%20full.webp)![](https://cdn.prod.website-files.com/67a1e6de2f2eab2e125f8b9a/68b70bf16dac55b0a9f0c26f_Blog-How%20to%20Run%20OpenAI%E2%80%99s%20Whisper%20Speech%20Recognition%20Model.png)\nSeptember 2, 2025\n### How to run OpenAI’s Whisper speech recognition model\n[](https://assemblyai.com/blog/how-to-run-openais-whisper-speech-recognition-model)\nBy\nRyan O&#x27;Connor\n,\nSenior Developer Educator\n[Tutorial](https://assemblyai.com/topic/tutorial)\n[Whisper](https://assemblyai.com/topic/whisper)\n[Python](https://assemblyai.com/topic/python)\n![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67a0fb99445797bcd265154a_16x9%20cover%20full.webp)![](https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67a0fb99445797bcd265154a_16x9%20cover%20full.webp)\nAugust 12, 2024\n### Introducing the AssemblyAI Ruby SDK\n[](https://assemblyai.com/blog/announcing-ruby-sdk)\nBy\nNiels Swimberghe\n,\n[Ruby](https://assemblyai.com/topic/ruby)\n[SDK](https://assemblyai.com/topic/sdk)\n[](https://assemblyai.com/topic/llms)\nLLMs\n[](https://assemblyai.com/topic/llm-gateway)\nLLM Gateway","image":"https://cdn.prod.website-files.com/67a1e6de2f2eab2e125f8b9a/68ff7ef061961fa81e329d42_Blog-LLM%20Gateway.png","favicon":"https://cdn.prod.website-files.com/67a08d9d7d19f8fb63692894/67e119e1e653ef1ce8e17a50_aai-favicon-32x32.png"}],"searchTime":1004.2,"costDollars":{"total":0.015,"search":{"neural":0.005},"contents":{"text":0.01}}}
